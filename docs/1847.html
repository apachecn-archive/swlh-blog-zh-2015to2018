<html>
<head>
<title>Shannon entropy in the context of machine learning and AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习和人工智能背景下的香农熵</h1>
<blockquote>原文：<a href="https://medium.com/swlh/shannon-entropy-in-the-context-of-machine-learning-and-ai-24aee2709e32?source=collection_archive---------1-----------------------#2018-01-04">https://medium.com/swlh/shannon-entropy-in-the-context-of-machine-learning-and-ai-24aee2709e32?source=collection_archive---------1-----------------------#2018-01-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="fef9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我想在机器学习和人工智能的背景下阐述香农熵的概念。我的目标是对香农熵背后的数学原理提供一些见解，但保持讨论相对非正式。</p><p id="eac3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以让我们开始吧。在机器学习中，量化与随机事件相关的预期信息量，以及量化概率分布之间的相似性是很常见的。在这两种情况下，香农熵都被用作概率分布的信息量的度量。它是以信息论之父克劳德·香农(1916-2001)的名字命名的。</p><h2 id="70a4" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">自我信息</h2><p id="1df6" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">香农熵背后的基本概念是一个事件的所谓自我信息，有时被称为惊奇。自我信息背后的直觉如下。当观察到一个事件的不太可能的结果(随机变量)时，我们将其与大量信息联系起来。相反，当观察到更有可能的结果时，我们会将其与更少量的信息联系起来。把自我信息想象成与某个事件相关的惊喜是非常有帮助的。例如，考虑一个总是正面朝上的严重倾斜的硬币。任何抛硬币的结果都是完全可以预测的，因此我们永远不会对结果感到惊讶，这意味着我们从这样的实验中得不到任何信息。换句话说，它的自我信息是零。如果硬币偏向较轻，每次我们扔硬币时都会有一些惊喜，尽管超过50%的时候我们仍然会看到正面。因此，自我信息大于零。如果硬币总是落在背面(而不是正面)，我们又会以零熵或惊喜告终。最大的惊喜是在一个无偏的硬币的情况下获得的，其中正面或反面落地的机会都是50%，因为这是投掷硬币的结果最难预测的情况。</p><p id="1771" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基于上述非正式要求，我们可以找到一个合适的函数来描述自我信息。对于可能取值为x_1，.。。，x_n和概率质量函数P(X)，在0和1之间的任何正的单调递减函数I(p_i)都可以用作信息的度量。到目前为止一切顺利。然而，另一个重要的性质是独立事件的可加性；后续两次抛硬币的自我信息应该是单次抛硬币的自我信息的两倍。这对于独立变量是有意义的，因为在这种情况下，惊奇或不可预测性的数量会增加两倍。形式上，我们需要I(p_i p_j) = I(p_i)+I(p_j)对于独立事件x_i和x_j .满足所有这些要求的函数是负对数，</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div class="er es ke"><img src="../Images/6137961277e229f3fc925f62b1ce6e06.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*M61jMpKOcTRJTRMiFFDwxQ@2x.png"/></div></figure><p id="a880" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">图1显示了I(p)的曲线图。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es km"><img src="../Images/0d30549de4733b3a3f7fa194f8e48a01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SyomXV4fILTT-Hc5uXurbg@2x.png"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx">Figure 1. The self-information function I(p). Low probabilities are associated with high self-information, and vice versa.</figcaption></figure><p id="60fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们回到简单的抛硬币实验。在信息论的语言中，需要一位(也称为“香农”)信息来表示一次抛硬币的两种可能结果。类似地，对于两次连续的掷硬币，需要两位信息来描述所有四种可能的结果。一般来说，需要log_2(n)比特的信息来描述n个连续的独立随机事件的结果，或者等价地，自我信息。让我们通过简单的计算来完成随后的三次抛硬币。有2^3 = 8种可能的结果，任何特定结果的概率是0.5^3 = 0.125。所以这个实验的自我信息是I(0.125)=-log _ 2(0.125)= 3。我们需要3位来表示所有可能的结果，因此任何特定的三次掷硬币序列的自我信息等于3.0。</p><p id="7ad6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们也可以计算一个连续随机变量的自信息。图2显示了三种不同的pdf及其相应的信息内容。2a中的Dirac delta对应于强偏置、零熵的情况，偏置硬币总是落在同一侧。每当p(x) = 0时，将关联无限高的信息内容。然而，这是假设性的，因为这些事件实际上不会发生，因为概率为零。图2B中的高斯概率密度函数类似于有偏向的硬币，硬币往往落在同一边，但不总是如此。最后，图2C描绘了一个统一的pdf，具有相关的统一信息内容，类似于我们的无偏掷硬币。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kv"><img src="../Images/b3d0dabe8428d263812341303a347fc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*USFoTUyFmiEtXTmLEtRG1w.png"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx">Figure 2. Three different probability densities p on [−3,+3] and their self-information I(p). (A) Dirac delta function (completely deterministic) (B) Gaussian with μ = 0,σ = 0.5 (biased towards x = 0) (C.) Uniform distribution.</figcaption></figure><h2 id="1b6f" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">熵</h2><p id="c5e2" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">到目前为止，我们只讨论了自我信息。对于公平硬币的情况，自我信息实际上等于香农熵，因为所有结果在概率上是相等的。然而，一般来说，香农熵是X的所有可能值的平均自我信息(期望值),</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kw"><img src="../Images/ef716c3b295a72581292ac9692bfc8a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pXcsUyiIsPJDU_ruWkgpBg@2x.png"/></div></div></figure><p id="cfce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中b是对数的底数。上面我们使用了b = 2，其他常见的选择是b = 10以及欧拉数e，但是这个选择并不重要，因为不同基数的对数由一个常数联系起来。我们从这里开始假设基数为2，省略b。</p><p id="5206" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你密切关注，你可能想知道当p(x_i) = 0时熵发生了什么，因为在这种情况下我们必须计算0 log(0)。原来<br/> lim p→0 p log(p) = 0。一个数学证明可以用洛必达的<a class="ae jd" href="http://tutorial.math.lamar.edu/Classes/CalcI/LHospitalsRule.aspx" rel="noopener ugc nofollow" target="_blank">法则建立。</a></p><p id="b018" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">香农熵推广到连续域，在那里它被称为微分熵(<a class="ae jd" href="https://en.wikipedia.org/wiki/Differential_entropy" rel="noopener ugc nofollow" target="_blank">一些限制适用</a>，这里将不讨论)。对于连续随机变量x和概率密度函数p(x)，香农熵定义如下:</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/5cae6df2831f74fda991b17126bf7491.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o4T0pgqjCYoxkvZpQ4j9bg@2x.png"/></div></div></figure><p id="db66" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的三个示例分布的熵是0(狄拉克δ)、174(高斯)和431(均匀)。从我们的实验中出现的模式是，宽分布具有最高的熵。仔细看看图2B和2C对你的理解很重要。尽管在高斯情况下，I(p)下的面积比均匀情况下大得多，但是熵比均匀情况下小得多，因为I(p)由概率密度函数p加权，概率密度函数p在高斯两边都接近于零。记住宽概率密度具有高熵的最好类比是想象一个装满气体的容器。我们从物理学中知道，一个封闭系统中的熵会随着时间的推移而增加，而不会自行减少。当我们在罐的任一侧注入气体后，气体粒子的分布(每单位体积的气体粒子数)在罐内收敛到一个统一的值。低熵将意味着更高密度的气体粒子在某些区域聚集，这种情况永远不会自行发生。许多气体粒子在一个小区域中的累积对应于我们的高斯pdf，在一个极端的情况下，对应于狄拉克δ函数，此时所有的气体粒子都被凝聚到一个无限小的区域中。</p><h2 id="8ad5" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">交叉熵</h2><p id="e255" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">交叉熵是一种比较两个概率分布p和q的数学工具，它类似于熵，但是我们不是计算p下log(p)的期望，而是计算p下log(q)的期望，</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es ky"><img src="../Images/e06173b1926f0ee2e45e14cce65b0ffe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uA6LE5g6egD9iBPksvu6QA@2x.png"/></div></div></figure><p id="50ee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在信息论的语言中，如果我们使用“错误的”编码方案q而不是p，这个量给我们从q编码一个事件所需的平均比特数。在机器学习中，它是概率分布相似性的一个非常有用的度量，并充当损失函数(下面有更多详细信息)。</p><h1 id="6659" class="kz jf hi bd jg la lb lc jk ld le lf jo lg lh li jr lj lk ll ju lm ln lo jx lp bi translated">机器学习中的用途</h1><p id="3222" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">在这一点上，你可能想知道熵与机器学习有什么关系。接下来让我们来看看一些特定的领域。</p><h2 id="4fae" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">贝叶斯学习</h2><p id="ac34" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">首先，上述高斯情况很重要，因为正态分布是机器学习应用中非常常见的建模选择。机器学习的目标是减少熵。我们想要做预测，我们必须对我们的预测有信心。熵给了我们一种量化这种信心的方法。在贝叶斯设置中，通常假设先验分布具有较宽的pdf，反映了在进行观察之前随机变量的值的不确定性。当数据进来时，熵减少并导致后验概率在参数的可能值周围形成峰值。</p><h2 id="3108" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">决策树学习</h2><p id="7e95" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">在决策树学习中，熵用于构建树。构建决策树从根节点开始，根据“最佳”属性的所有可能值，即最小化结果子集中的(组合)熵的值，将数据集S分成多个子集。这个过程递归地重复，直到没有更多的属性留下来分裂。这个程序叫做<a class="ae jd" href="https://en.wikipedia.org/wiki/ID3_algorithm" rel="noopener ugc nofollow" target="_blank"> ID3算法</a>。</p><h2 id="e530" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">分类</h2><p id="67cd" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">在二项式和多项式分类方案中，交叉熵是逻辑回归和神经网络的标准损失函数的基础。通常，p用于真实(或经验)分布(即训练集的分布)，而q是由模型描述的分布。让我们以二元逻辑回归为例。这两个类别被标记为0和1，逻辑模型将概率q_(y=1) = ŷ和q _(y = 0)= 1ŷ分配给每个输入x。这可以简单地写成q ∈ {ŷ，1 ŷ}.尽管经验标签p总是恰好为0或1，但这里使用了相同的符号p ∈ {y，1y }，所以不要混淆。使用这种符号，单个样本的经验分布和估计分布之间的交叉熵为</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es lq"><img src="../Images/4d75d2d3526295f590b9339a15e32304.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ayisu1Zs2t_7tPBEyyAgGg@2x.png"/></div></div></figure><p id="4315" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当用作损失函数时，使用来自所有N个样本的所有交叉熵的平均值，</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es lr"><img src="../Images/cd88dc3a651062d62043fc6baf2222f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3LkITokhU5tKkivVn2ftdw@2x.png"/></div></div></figure><h2 id="0357" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">KL散度</h2><p id="362f" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">与交叉熵密切相关的是，从q到p的KL散度，写为DKL(p||q)，是机器学习中经常使用的另一种相似性度量。在贝叶斯推理的语言中，DKL(p||q)是当一个人将自己的信念从先验分布q修改为后验分布p时获得的信息的度量，或者换句话说，是当q用于逼近p时丢失的信息量。例如，它用于训练变分自动编码器的潜在空间表示。KL散度可以用熵和交叉熵来表示，</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div class="er es ls"><img src="../Images/d56f2c127e985b8c3f9e2bd616d5efb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*faNf4hDTT9KMCCrI29PIfg@2x.png"/></div></figure><p id="2684" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当使用针对q优化的编码方案时，交叉熵测量对来自p的事件进行编码所需的平均总比特数，而KL散度给出了当使用针对q的最优编码方案而不是针对p的最优编码方案时所需的额外比特数。由此我们可以看出，在机器学习的上下文中，p是固定的，交叉熵和KL散度通过常数加法项相关，因此为了优化的目的，它们是等价的。从理论的角度讲KL散度而不是交叉熵可能还是有意义的，KL散度的一个有用的性质是当p和q相等时为零。</p><h1 id="dfda" class="kz jf hi bd jg la lb lc jk ld le lf jo lg lh li jr lj lk ll ju lm ln lo jx lp bi translated">结论</h1><p id="d1e9" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">这篇文章的目的是在机器学习和人工智能的背景下，通过解释该理论最重要的方面，以及它在应用中出现的位置，来阐明熵的概念。用于生成上述所有结果的Python代码可以在这个要点的<a class="ae jd" href="https://gist.github.com/fpreiswerk/5cd455d157249a10e874ec91d2e48593" rel="noopener ugc nofollow" target="_blank">中找到。</a></p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es lt"><img src="../Images/731acf26f5d44fdc58d99a6388fe935d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6gfnVvkMRFtjVsWF7vkClA.png"/></div></div></figure><h2 id="fd41" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">这篇文章发表在<a class="ae jd" href="https://medium.com/swlh" rel="noopener">《创业</a>》上，这是Medium最大的创业刊物，有281，454+人关注。</h2><h2 id="7945" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">订阅接收<a class="ae jd" href="http://growthsupply.com/the-startup-newsletter/" rel="noopener ugc nofollow" target="_blank">我们的头条新闻</a>。</h2><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es lt"><img src="../Images/731acf26f5d44fdc58d99a6388fe935d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6gfnVvkMRFtjVsWF7vkClA.png"/></div></div></figure></div></div>    
</body>
</html>
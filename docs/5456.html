<html>
<head>
<title>Introduction to Reinforcement Learning (Coding Q-Learning) — Part 3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习介绍(编码Q学习)——第三部分</h1>
<blockquote>原文：<a href="https://medium.com/swlh/introduction-to-reinforcement-learning-coding-q-learning-part-3-9778366a41c0?source=collection_archive---------0-----------------------#2018-07-09">https://medium.com/swlh/introduction-to-reinforcement-learning-coding-q-learning-part-3-9778366a41c0?source=collection_archive---------0-----------------------#2018-07-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/a5950344bb4e1675796b241d64d4fc06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7aTsPF6H4xFneBod8UOUcw.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Talk is cheap. Show me the code — Linus Torvalds</figcaption></figure><blockquote class="iu"><p id="8f9f" class="iv iw hi hj ix iy iz ja jb jc jd je dx translated">在前一部分中，我们看到了什么是MDP，什么是Q学习。现在，在这一部分，我们将看到如何解决一个有限的MDP使用Q学习和编码。</p></blockquote><h1 id="d8d2" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated">奥鹏健身馆</h1><p id="e9ba" class="pw-post-body-paragraph kd ke hi kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz je hb bi translated">如OpenAI健身房<a class="ae la" href="https://gym.openai.com/" rel="noopener ugc nofollow" target="_blank">官网</a>所述:</p><blockquote class="lb lc ld"><p id="d521" class="kd ke le kf b kg lf ki kj kk lg km kn lh li kq kr lj lk ku kv ll lm ky kz je hb bi translated">Gym是一个开发和比较强化学习算法的工具包。</p></blockquote><p id="c7cf" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">我们将使用这个工具包来解决<a class="ae la" href="https://gym.openai.com/envs/FrozenLake-v0" rel="noopener ugc nofollow" target="_blank">冰冻湖</a>的环境。有各种各样的游戏可用，如雅达利2600的，基于文本的游戏等。点击查看全部<a class="ae la" href="https://gym.openai.com/envs/" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="b028" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq ln js jt ju lo jw jx jy lp ka kb kc bi translated">装置</h1><p id="058d" class="pw-post-body-paragraph kd ke hi kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz je hb bi translated">使用此处提供的<a class="ae la" href="https://github.com/openai/gym" rel="noopener ugc nofollow" target="_blank">步骤安装gym。首先安装体育馆库，然后安装操作系统特定的包。</a></p><p id="1f9d" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">现在，让我们看看如何使用健身房工具包。</p><p id="e1f6" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">导入健身房使用:</p><pre class="lq lr ls lt fd lu lv lw lx aw ly bi"><span id="8c26" class="lz jg hi lv b fi ma mb l mc md">import gym</span></pre><p id="85cb" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">然后，指定你想要使用的游戏，我们将使用FrozenLake游戏。</p><pre class="lq lr ls lt fd lu lv lw lx aw ly bi"><span id="bcfd" class="lz jg hi lv b fi ma mb l mc md">env = gym.make('FrozenLake-v0')</span></pre><p id="97b5" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">游戏的环境可以被重置为默认/初始状态，使用:</p><pre class="lq lr ls lt fd lu lv lw lx aw ly bi"><span id="bd3c" class="lz jg hi lv b fi ma mb l mc md">env.reset()</span></pre><p id="5592" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">要查看游戏GUI，请使用:</p><pre class="lq lr ls lt fd lu lv lw lx aw ly bi"><span id="c103" class="lz jg hi lv b fi ma mb l mc md">env.render()</span></pre><p id="cada" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">官方文档可以在<a class="ae la" href="http://gym.openai.com/docs/" rel="noopener ugc nofollow" target="_blank">这里</a>找到，在这里你可以看到gym toolkit的详细用法和解释。</p><h1 id="d97b" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq ln js jt ju lo jw jx jy lp ka kb kc bi translated">冰冻湖游戏</h1><p id="6964" class="pw-post-body-paragraph kd ke hi kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz je hb bi translated">现在，让我们看看什么是冰冻湖游戏。</p><p id="df44" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">想象一下，你站在一个结冰的湖上。这个湖并不都结冰了，有些地方冰很薄。你的目标是从地点S到G，不要掉进洞里。</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es me"><img src="../Images/54ab21e1397f6a6ea882bed2c7da9798.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*Qp14HWQfOeE2UoSxrxCxAg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx"><a class="ae la" href="https://www.google.co.in/url?sa=i&amp;rct=j&amp;q=&amp;esrc=s&amp;source=images&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwjnpe-owY_cAhUGVH0KHVyTAYAQjRx6BAgBEAU&amp;url=https%3A%2F%2Fanalyticsindiamag.com%2Fopenai-gym-frozen-lake-beginners-guide-reinforcement-learning%2F&amp;psig=AOvVaw095bfUMvJGKysgeHZjz6ZT&amp;ust=1531139425155647" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="4813" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">这里，S是起点，G是目标，F是代理人可以站立的固体冰，H是如果代理人去，它会掉下来的洞。</p><p id="0f0b" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">代理有4种可能的移动，在环境中分别表示为0、1、2、3，分别表示向左、向右、向下、向上。</p><p id="fe0f" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">对于每个状态F，代理人获得0奖励，对于状态H，它获得-1奖励，因为在状态H中，代理人将死亡，并且在达到目标时，代理人获得+1奖励。</p><p id="cefb" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">游戏在终端中呈现后看起来像这样:</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/6ecbd57a7628c22fd3a719467619a33a.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/1*Zf_ozFRh4ZqhOI-RfkZfxA.gif"/></div><figcaption class="iq ir et er es is it bd b be z dx">FrozenLake-v0</figcaption></figure><p id="d162" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">这里的状态是F、S和g，也就是说有4x4=16个状态和4个动作。</p><p id="ba05" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">为了使用Q-learning解决这个游戏，我们将利用我们在<a class="ae la" rel="noopener" href="/@adeshg7/introduction-to-reinforcement-learning-part-2-74e0a3fad9d3">上一部分</a>中看到的理论。</p><p id="6414" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">这是使用Q-learning解决“FrozenLake-v0”环境的代码。这是非常直接的，你会觉得很舒服，直到文章结束。</p><figure class="lq lr ls lt fd ij"><div class="bz dy l di"><div class="mg mh l"/></div></figure><p id="240f" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">我们来解剖一下。</p><p id="b19c" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">第1–3行是导入我们将使用的库。<strong class="kf mi"> Numpy </strong>用于存储Q表，而<strong class="kf mi"> pickle </strong>用于将我们的Q表保存为“pkl”文件。</p><p id="afcd" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">第5行初始化FrozenLake环境。</p><p id="84b4" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">第7–12行初始化我们的变量。<strong class="kf mi"><em class="le">ε</em></strong>对于ε-贪婪方法，<strong class="kf mi"><em class="le">γ</em></strong>是折扣因子，<strong class="kf mi"><em class="le">max _剧集</em> </strong>是我们将运行游戏的最大次数，<strong class="kf mi"> <em class="le"> max_steps </em> </strong>是我们将运行每一集的最大步数，<strong class="kf mi"> <em class="le"> lr_rate </em> </strong>是学习率。</p><p id="2601" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">第14行将我们的Q-table初始化为一个填充了零的16x4矩阵。<strong class="kf mi"><em class="le">env . observation-space . n</em></strong>告知游戏中状态总数<strong class="kf mi"><em class="le">env . action-space . n</em></strong>告知动作总数。</p><p id="63b1" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">在第30行，我们开始播放剧集。</p><p id="1bd3" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">在第31行，变量<strong class="kf mi"> <em class="le"> state </em> </strong>使用<strong class="kf mi"> <em class="le"> env.reset() </em> </strong>存储初始状态。</p><p id="6c7c" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">在第32行，<strong class="kf mi"> <em class="le"> t </em> </strong>用于存储时间步数。</p><p id="590f" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">使用第35行，环境被渲染。</p><p id="b04c" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">在第37行，选择适当的动作。这是使用<strong class="kf mi"><em class="le">ε贪婪</em> </strong>方法完成的。参见第16–22行，这里我们随机生成一个0到1之间的数，看看它是否小于ε。如果它较小，那么使用<strong class="kf mi"><em class="le">env . action _ space . sample()</em></strong>选择一个随机动作，如果它较大，那么我们选择在Q表中具有最大值的动作作为状态:<strong class="kf mi"> <em class="le">状态</em> </strong>。例如:</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/2aa78667cc37ae74de001d231fb958b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*gi3FZfY0AG8nHrSxksS4qw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">State 10 with q values</figcaption></figure><p id="c7ea" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">假设，对于状态10中的动作0–3，其值为0.33、0.34、0.79和0.23。对于动作2，最大Q值是0.79，并且该动作2被选择用于状态10。</p><p id="7ca6" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">在第39行，我们选择的行为发生在环境中，下一个状态，行为的奖励被返回。<strong class="kf mi"> <em class="le"> done </em> </strong>如果剧集已经终止，则返回true，并且<strong class="kf mi"> <em class="le"> info </em> </strong>存储用于调试的额外信息。</p><p id="9326" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">此时，我们有:</p><p id="9e1a" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">1.先前的<strong class="kf mi">状态，<em class="le">状态，</em>状态，</strong>。</p><p id="3b29" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">2.下一个状态，<strong class="kf mi"> <em class="le"> state2。</em> </strong></p><p id="4e93" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">3.而<strong class="kf mi"> <em class="le">动作</em></strong><strong class="kf mi"><em class="le">奖励</em> </strong>为<strong class="kf mi"> <em class="le">状态2 </em> </strong>。</p><p id="1d9d" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">在第41行，我们使用上面的信息通过下面的等式使用函数<strong class="kf mi"> <em class="le"> learn(state，state2，reward，action) </em> </strong>来更新我们的Q表:</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mk"><img src="../Images/bb9acbf5dac5d766645e795777f96b9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Lq_5xHxRdXRBlYIBCEssg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Q-value updation equation</figcaption></figure><p id="1942" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">在更新Q-table后，我们将第43行的<strong class="kf mi"> <em class="le">状态、</em> </strong>状态设置为当前状态、<strong class="kf mi"> <em class="le">状态2 </em> </strong>。</p><p id="6a96" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">第45行的时间步长增加。第47–48行检查done是否为真，也就是说，这一集是否结束。</p><p id="57da" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">在第54和55行，我们将Q表保存到<em class="le">“frozen lake _ Q table . pkl”文件中。</em></p><p id="a91a" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">仅此而已。请随意摆弄代码。</p><h1 id="505a" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq ln js jt ju lo jw jx jy lp ka kb kc bi translated">FrozenLake在行动</h1><p id="415b" class="pw-post-body-paragraph kd ke hi kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz je hb bi translated">运行上面的代码，你会看到游戏在运行。但是，请耐心等待，因为完成10000集需要一些时间。</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ml"><img src="../Images/c87a66270519fd064ff139df18c47102.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*jQOFmSzotNW4p4U2g19GkA.gif"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Agent in action</figcaption></figure><p id="250e" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">你可以在之后加载Q-table并使用下面的代码玩游戏。这相当简单。下面的代码中只删除了培训部分。</p><figure class="lq lr ls lt fd ij"><div class="bz dy l di"><div class="mg mh l"/></div></figure><p id="e7be" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">请继续关注强化学习的更多乐趣。快乐探索😄。</p><p id="4014" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">请点击👏按钮，如果你喜欢这个帖子，拿着它给更多的爱。</p><p id="5b2d" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated">如果您希望连接:</p><p id="92ca" class="pw-post-body-paragraph kd ke hi kf b kg lf ki kj kk lg km kn ko li kq kr ks lk ku kv kw lm ky kz je hb bi translated"><a class="ae la" href="https://twitter.com/gautamades" rel="noopener ugc nofollow" target="_blank"> <strong class="kf mi">推特</strong></a><strong class="kf mi"/><a class="ae la" href="https://www.instagram.com/adeshgautam/" rel="noopener ugc nofollow" target="_blank"><strong class="kf mi">insta gram</strong></a><strong class="kf mi"/><a class="ae la" href="https://www.linkedin.com/in/adesh-gautam-518810127/" rel="noopener ugc nofollow" target="_blank"><strong class="kf mi">LinkedIn</strong></a><strong class="kf mi"/><a class="ae la" href="https://github.com/adesgautam" rel="noopener ugc nofollow" target="_blank"><strong class="kf mi">Github</strong></a></p><figure class="lq lr ls lt fd ij er es paragraph-image"><a href="https://medium.com/swlh"><div class="er es mm"><img src="../Images/308a8d84fb9b2fab43d66c117fcc4bb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YqDjlKFwScoQYQ62DWEdig.png"/></div></a></figure><h2 id="48f8" class="lz jg hi bd jh mn mo mp jl mq mr ms jp ko mt mu jt ks mv mw jx kw mx my kb mz bi translated">这个故事发表在<a class="ae la" href="https://medium.com/swlh" rel="noopener"> The Startup </a>上，这是Medium最大的企业家出版物，拥有343，876+人。</h2><h2 id="24b8" class="lz jg hi bd jh mn mo mp jl mq mr ms jp ko mt mu jt ks mv mw jx kw mx my kb mz bi translated">在这里订阅接收<a class="ae la" href="http://growthsupply.com/the-startup-newsletter/" rel="noopener ugc nofollow" target="_blank">我们的头条新闻</a>。</h2><figure class="lq lr ls lt fd ij er es paragraph-image"><a href="https://medium.com/swlh"><div class="er es mm"><img src="../Images/b0164736ea17a63403e660de5dedf91a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ouK9XR4xuNWtCes-TIUNAw.png"/></div></a></figure></div></div>    
</body>
</html>
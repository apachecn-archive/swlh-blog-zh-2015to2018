<html>
<head>
<title>Bag of Tricks for Image Classification with Convolutional Neural Networks in Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Keras中卷积神经网络用于图像分类的技巧包</h1>
<blockquote>原文：<a href="https://medium.com/swlh/bag-of-tricks-for-image-classification-with-convolutional-neural-networks-in-keras-ff99d0ef68f9?source=collection_archive---------5-----------------------#2018-12-31">https://medium.com/swlh/bag-of-tricks-for-image-classification-with-convolutional-neural-networks-in-keras-ff99d0ef68f9?source=collection_archive---------5-----------------------#2018-12-31</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/1f3a6ce12a5a9c72111e77c2ca95eb41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DDYBOOFwW9mNo4JM.png"/></div></div></figure><p id="b780" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这篇教程向你展示了如何在Keras API中实现一些图像分类任务的技巧，如https://arxiv.org/abs/1812.01187v2<a class="ae jo" href="https://arxiv.org/abs/1812.01187v2" rel="noopener ugc nofollow" target="_blank">的论文</a>中所示。这些技巧适用于各种CNN模型，如ResNet-50、Inception-V3和MobileNet。</p><h1 id="6579" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">大批量训练</h1><p id="6172" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">对于相同数量的时期，与使用较小批量训练的模型相比，使用较大批量训练的模型会导致验证准确性下降。四种启发法，有助于最大限度地减少大批量训练的负面影响，提高准确性和训练速度。</p><h1 id="ffb8" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">线性比例学习率</h1><p id="ab06" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">随着批量大小线性增加学习速率</p><p id="36f0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">例如</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es ks"><img src="../Images/307ed73eb4e8443016d3bb5bb9df3e17.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*o5QxSeTr8aPndT2udah9EA.png"/></div></figure><p id="d44d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在Keras API中，您可以像这样缩放学习速率和批量大小。</p><figure class="kt ku kv kw fd ij"><div class="bz dy l di"><div class="kx ky l"/></div></figure><h1 id="7a0b" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">学习率热身</h1><p id="6f08" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">使用太大的学习率可能导致数值不稳定，特别是在训练的最开始，参数是随机初始化的。预热策略在初始<strong class="is kz"> <em class="la"> N </em> </strong>时期或<strong class="is kz"> <em class="la"> m </em> </strong>批次期间将学习率从0线性增加到初始学习率。</p><p id="555e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">尽管Keras自带<a class="ae jo" href="https://keras.io/callbacks/#learningratescheduler" rel="noopener ugc nofollow" target="_blank"> LearningRateScheduler </a>能够更新每个训练时期的学习率，但为了更好地更新每个批次，这里介绍了如何实现定制的Keras回调来实现这一点。</p><figure class="kt ku kv kw fd ij"><div class="bz dy l di"><div class="kx ky l"/></div></figure><p id="5196" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><code class="du lb lc ld le b">warm_up_lr.learning_rates</code>现在包含了每个训练批次的预定学习率的数组，让我们将它可视化。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es lf"><img src="../Images/4eb2df02e1fb13379382649895080567.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/0*u--UK5AtBt-2rCGm.png"/></div></figure><h1 id="e892" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">每个ResNet块的零γ最后一批规范化层</h1><p id="9c28" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">批处理规范化使用γ缩放一批输入，使用β进行移位，γ和β都是可学习的参数，默认情况下，它们的元素在Keras中分别初始化为1和0。</p><p id="8420" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在零γ初始化试探法中，我们为位于残差块末端的所有BN层初始化γ = 0。因此，所有残差块仅返回它们的输入，模仿具有较少层数并且在初始阶段更容易训练的网络。</p><p id="d566" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">给定一个identity ResNet块，当最后一个BN的γ初始化为零时，这个块将只把快捷方式输入传递给下游层。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lg"><img src="../Images/b77487f55afa7786ea574e2e209b2f2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KfEszFYh-wrl0OeV.png"/></div></div></figure><p id="ad48" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你可以看到这个ResNet块是如何在Keras中实现的，唯一的变化是行，<a class="ae jo" href="https://keras.io/layers/normalization/#batchnormalization" rel="noopener ugc nofollow" target="_blank"> BatchNormalization </a>层的<code class="du lb lc ld le b">gamma_initializer='zeros'</code>。</p><figure class="kt ku kv kw fd ij"><div class="bz dy l di"><div class="kx ky l"/></div></figure><h1 id="8ce4" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">无偏差衰减</h1><p id="22db" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">将L2正则化应用于所有参数的标准权重衰减将它们的值推向0。它包括对层权重应用惩罚。然后将惩罚应用于损失函数。</p><p id="a23f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">建议仅对权重应用正则化，以避免过度拟合。其他参数，包括BN层中的偏压和γ和β，保持不变。</p><p id="b219" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在Keras中，将L2正则化应用于核权重是毫不费力的。选项<strong class="is kz">bias _ regulator</strong>也可用，但不推荐使用。</p><figure class="kt ku kv kw fd ij"><div class="bz dy l di"><div class="kx ky l"/></div></figure><h1 id="adbb" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">训练改进</h1><h1 id="3ec3" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">余弦学习率衰减</h1><p id="1cbb" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">在前面描述的学习率预热阶段之后，我们通常会从初始学习率开始稳步降低其值。与包括指数衰减和阶跃衰减在内的一些广泛使用的策略相比，余弦衰减在开始时缓慢降低学习速率，然后<br/>在中间变得几乎线性降低，并在结束时再次减慢。它有可能提高训练进度。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es lh"><img src="../Images/15363db34f4ab6f53a038a46ede12bef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/0*40bLqKrctZHF-jJi.png"/></div></figure><p id="1dd1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是一个完整的带有预热阶段的余弦学习率调度程序的例子。在Keras中，调度程序在每个更新步骤的粒度上更新学习率。</p><figure class="kt ku kv kw fd ij"><div class="bz dy l di"><div class="kx ky l"/></div></figure><p id="a6a3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">您选择使用调度器中的<strong class="is kz"> hold_base_rate_steps </strong>参数，顾名思义，该参数在进行余弦衰减之前保存特定步数的基本学习速率。由此产生的学习率时间表将有一个平台，如下所示。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es li"><img src="../Images/0166a5c5db86216417901e93aa10f16a.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/0*YjAOKiE7tuygNS6h.png"/></div></figure><h1 id="9326" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">标签平滑</h1><p id="171e" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">与原始的独热编码输入相比，标签平滑改变了真实概率的构造，</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es lj"><img src="../Images/a6d854f019cee3214ad2f83bcb7bae7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/0*u8AwutOlFRfpnOZE.png"/></div></figure><p id="91aa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">其中ε是一个小常数。标注平滑鼓励全连接图层的有限输出，使模型更好地进行概化，并且不容易过度拟合。对于标签噪声，这也是一种有效且理论上可行的解决方案。你可以在这里阅读更多关于<a class="ae jo" href="https://qr.ae/TUnRbn" rel="noopener ugc nofollow" target="_blank">讨论</a>的内容。<br/>以下是在训练分类器之前，如何对单热点标签应用标签平滑。</p><figure class="kt ku kv kw fd ij"><div class="bz dy l di"><div class="kx ky l"/></div></figure><p id="0ece" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">结果</p><pre class="kt ku kv kw fd lk le ll lm aw ln bi"><span id="d848" class="lo jq hi le b fi lp lq l lr ls">Before smoothing: [<strong class="le kz">0.</strong> <strong class="le kz">0.</strong> <strong class="le kz">0.</strong> <strong class="le kz">0.</strong> <strong class="le kz">0.</strong> <strong class="le kz">1.</strong> <strong class="le kz">0.</strong> <strong class="le kz">0.</strong> <strong class="le kz">0.</strong> <strong class="le kz">0.</strong>]<br/>After smoothing: [<strong class="le kz">0.01</strong>  <strong class="le kz">0.01</strong>  <strong class="le kz">0.01</strong>  <strong class="le kz">0.01</strong>  <strong class="le kz">0.01</strong>  <strong class="le kz">0.90999997</strong>  <strong class="le kz">0.01</strong>  <strong class="le kz">0.01</strong>  <strong class="le kz">0.01</strong>  <strong class="le kz">0.01</strong>]</span></pre><h1 id="595d" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">结论和进一步阅读</h1><p id="8e31" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">在这篇文章中，还有两个培训优化没有涉及到，即，</p><ul class=""><li id="4ace" class="lt lu hi is b it iu ix iy jb lv jf lw jj lx jn ly lz ma mb bi translated"><strong class="is kz">知识提炼</strong>利用预先训练的较大模型的输出来训练较小的模型。</li><li id="cad0" class="lt lu hi is b it mc ix md jb me jf mf jj mg jn ly lz ma mb bi translated"><strong class="is kz"> Mixup训练</strong>，某种意义上类似于增强，它通过对两个样本进行加权线性插值，形成新的样本，从而创造更多的数据。我们将在以后的文章中考虑实现这一点。</li></ul><p id="3594" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">阅读论文<a class="ae jo" href="https://arxiv.org/abs/1812.01187v2" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1812.01187v2</a>了解每一个技巧的详细信息。</p><p id="480e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我的GitHub上有源代码。</p><div class="mh mi ez fb mj mk"><a href="https://github.com/Tony607/Keras_Bag_of_Tricks" rel="noopener  ugc nofollow" target="_blank"><div class="ml ab dw"><div class="mm ab mn cl cj mo"><h2 class="hj b fi z dy mp ea eb mq ed ef hh bi translated">Tony607/Keras_Bag_of_Tricks</h2><div class="mr l"><h3 class="bd b fi z dy mp ea eb mq ed ef dx translated">Keras-Tony 607/Keras _ Bag _ of _ Tricks中卷积神经网络用于图像分类的技巧包</h3></div><div class="ms l"><p class="bd b fp z dy mp ea eb mq ed ef dx translated">github.com</p></div></div><div class="mt l"><div class="mu l mv mw mx mt my io mk"/></div></div></a></div><p id="ad03" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae jo" href="https://twitter.com/intent/tweet?url=https%3A//www.dlology.com/blog/bag-of-tricks-for-image-classification-with-convolutional-neural-networks-in-keras/&amp;text=Bag%20of%20Tricks%20for%20Image%20Classification%20with%20Convolutional%20Neural%20Networks%20in%20Keras" rel="noopener ugc nofollow" target="_blank">在Twitter上分享</a> <a class="ae jo" href="https://www.facebook.com/sharer/sharer.php?u=https://www.dlology.com/blog/bag-of-tricks-for-image-classification-with-convolutional-neural-networks-in-keras/" rel="noopener ugc nofollow" target="_blank">在脸书分享</a></p></div><div class="ab cl mz na gp nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="hb hc hd he hf"><p id="975b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">【www.dlology.com】最初发表于<a class="ae jo" href="https://www.dlology.com/blog/bag-of-tricks-for-image-classification-with-convolutional-neural-networks-in-keras/" rel="noopener ugc nofollow" target="_blank"><em class="la"/></a><em class="la">。</em></p><figure class="kt ku kv kw fd ij er es paragraph-image"><a href="https://medium.com/swlh"><div class="er es ng"><img src="../Images/308a8d84fb9b2fab43d66c117fcc4bb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YqDjlKFwScoQYQ62DWEdig.png"/></div></a></figure><h2 id="c148" class="lo jq hi bd jr nh ni nj jv nk nl nm jz jb nn no kd jf np nq kh jj nr ns kl nt bi translated">这篇文章发表在<a class="ae jo" href="https://medium.com/swlh" rel="noopener"> The Startup </a>上，这是Medium最大的创业刊物，拥有+405，714名读者。</h2><h2 id="869c" class="lo jq hi bd jr nh ni nj jv nk nl nm jz jb nn no kd jf np nq kh jj nr ns kl nt bi translated">在此订阅接收<a class="ae jo" href="http://growthsupply.com/the-startup-newsletter/" rel="noopener ugc nofollow" target="_blank">我们的头条新闻</a>。</h2><figure class="kt ku kv kw fd ij er es paragraph-image"><a href="https://medium.com/swlh"><div class="er es ng"><img src="../Images/b0164736ea17a63403e660de5dedf91a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ouK9XR4xuNWtCes-TIUNAw.png"/></div></a></figure></div></div>    
</body>
</html>
<html>
<head>
<title>How to train Keras model x20 times faster with TPU for free</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何免费用TPU训练Keras车型速度快20倍</h1>
<blockquote>原文：<a href="https://medium.com/swlh/how-to-train-keras-model-x20-times-faster-with-tpu-for-free-cac6cf5089cb?source=collection_archive---------4-----------------------#2018-10-16">https://medium.com/swlh/how-to-train-keras-model-x20-times-faster-with-tpu-for-free-cac6cf5089cb?source=collection_archive---------4-----------------------#2018-10-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/5ff3e377aa5a009b611bf4eb9a16d09b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pRGg-2hp9ik_M_Hi.png"/></div></div></figure><p id="1b68" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在相当长的一段时间里，我对在单个GTX 1070显卡上训练我的模型感到满意，该显卡的单次精度约为8.18 TFlops，然后谷歌在Colab上开放了免费的Tesla K80 GPU，该GPU配有12GB RAM，额定速度略快于8.73 TFlops。直到最近，具有180 TFlops的云TPU选项才在Colab的运行时类型选择器中弹出。在这个快速教程中，您将学习如何将您现有的Keras模型转化为TPU模型，并在Colab x20上进行训练，与在我的GTX1070上进行免费训练相比，速度更快。</p><p id="72c4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将构建一个易于理解但足够复杂的Keras模型，以便我们可以稍微预热一下云TPU。在IMDB情感分类任务上训练LSTM模型可能是一个很好的例子，因为训练LSTM可能比其他层(如密集层和卷积层)在计算上更昂贵。</p><p id="94f0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">工作流程的概述，</p><ul class=""><li id="9378" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">构建一个Keras模型，用于使用静态输入<code class="du jx jy jz ka b">batch_size</code>进行函数式API的培训。</li><li id="8e2d" class="jo jp hi is b it kb ix kc jb kd jf ke jj kf jn jt ju jv jw bi translated">将Keras模型转换为TPU模型。</li><li id="1d7c" class="jo jp hi is b it kb ix kc jb kd jf ke jj kf jn jt ju jv jw bi translated">用静态<code class="du jx jy jz ka b">batch_size * 8</code>训练TPU模型并将权重保存到文件中。</li><li id="cff5" class="jo jp hi is b it kb ix kc jb kd jf ke jj kf jn jt ju jv jw bi translated">构建一个Keras模型进行推理，具有相同的结构，但批量输入大小可变。</li><li id="d4a6" class="jo jp hi is b it kb ix kc jb kd jf ke jj kf jn jt ju jv jw bi translated">加载模型重量。</li><li id="7cb5" class="jo jp hi is b it kb ix kc jb kd jf ke jj kf jn jt ju jv jw bi translated">使用推理模型进行预测。</li></ul><h2 id="a5ee" class="kg kh hi bd ki kj kk kl km kn ko kp kq jb kr ks kt jf ku kv kw jj kx ky kz la bi translated">你可以一边阅读一边玩Colab Jupyter笔记本— <a class="ae lb" href="https://colab.research.google.com/drive/1QZf1WeX3EQqBLeFeT4utFKBqq-ogG1FN" rel="noopener ugc nofollow" target="_blank"> Keras_LSTM_TPU.ipynb </a>。</h2><p id="bca7" class="pw-post-body-paragraph iq ir hi is b it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj lg jl jm jn hb bi translated">首先，按照下图中的说明在Colab运行时激活TPU。</p><figure class="li lj lk ll fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lh"><img src="../Images/be099b2e5f07a92da959391ed9f838ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Oz9lwJ00YRq2IQI-.png"/></div></div><figcaption class="lm ln et er es lo lp bd b be z dx">Activate TPU</figcaption></figure><h1 id="c7e9" class="lq kh hi bd ki lr ls lt km lu lv lw kq lx ly lz kt ma mb mc kw md me mf kz mg bi translated">静态输入批量</h1><p id="473e" class="pw-post-body-paragraph iq ir hi is b it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj lg jl jm jn hb bi translated">在CPU和GPU上运行的输入管道大多不受静态形状要求的约束，而在XLA/TPU环境中，静态形状和批量大小是必须的。</p><p id="2972" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">can TPU包含8个TPU内核，作为独立的处理单元运行。除非使用全部八个内核，否则TPU不会得到充分利用。为了充分加快矢量化的训练速度，与在单个GPU上训练相同的模型相比，我们可以选择更大的批量。总批量大小为1024(每个内核128)通常是一个好的起点。</p><p id="e457" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果您要训练一个批量过大的较大模型，请尝试慢慢减少批量，直到它适合TPU内存，只是要确保总批量是64的倍数(每个内核的批量应该是8的倍数)。</p><p id="4cd5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在批量较大的训练时也值得一提；提高优化器的学习速度以允许更快的收敛通常是安全的。你可以在本文中找到一个参考——“<a class="ae lb" href="https://arxiv.org/pdf/1706.02677.pdf" rel="noopener ugc nofollow" target="_blank">精准，大迷你批量SGD:1小时训练ImageNet</a>”。</p><p id="8cff" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在Keras中，为了定义静态批量大小，我们使用其函数API，然后为输入层指定<code class="du jx jy jz ka b">batch_size</code>参数。请注意，该模型构建在一个采用<code class="du jx jy jz ka b">batch_size</code>参数的函数中，因此我们可以稍后回来创建另一个在CPU或GPU上运行的推理模型，该模型采用可变的批处理大小输入。</p><figure class="li lj lk ll fd ij"><div class="bz dy l di"><div class="mh mi l"/></div></figure><p id="cadd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">此外，使用<code class="du jx jy jz ka b">tf.train.Optimizer</code>代替标准的Keras优化器，因为Keras优化器对TPU的支持仍处于试验阶段。</p><h1 id="564f" class="lq kh hi bd ki lr ls lt km lu lv lw kq lx ly lz kt ma mb mc kw md me mf kz mg bi translated">将Keras模型转换为TPU模型</h1><p id="fb9a" class="pw-post-body-paragraph iq ir hi is b it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj lg jl jm jn hb bi translated"><code class="du jx jy jz ka b">tf.contrib.tpu.keras_to_tpu_model</code>功能将<code class="du jx jy jz ka b">tf.keras</code>模型转换成等效的TPU版本。</p><figure class="li lj lk ll fd ij"><div class="bz dy l di"><div class="mh mi l"/></div></figure><p id="8307" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后，我们使用标准的Keras方法来训练、保存权重和评估模型。请注意，<code class="du jx jy jz ka b">batch_size</code>设置为模型输入<code class="du jx jy jz ka b">batch_size</code>的八倍，因为输入样本均匀分布在8个TPU内核上运行。</p><figure class="li lj lk ll fd ij"><div class="bz dy l di"><div class="mh mi l"/></div></figure><p id="78ac" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我设置了一个实验来比较在我的Windows PC上运行的单个GTX1070和在Colab上运行的TPU之间的训练速度，下面是结果。</p><p id="6d09" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">GPU和TPU都采用128的输入批量，</p><p id="3df3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">GPU: <strong class="is mj">每历元179秒</strong>。20个时期达到76.9%的验证准确度，总共3600秒。</p><p id="9128" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">TPU: <strong class="is mj">每个时段5秒</strong>除了第一个时段用了49秒。20个时期达到95.2%的验证准确度，总共150秒。</p><p id="5861" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">20个历元后TPU的验证精度高于GPU可能是由于一次训练8批128个样本的小批量造成的。</p><h1 id="7b5a" class="lq kh hi bd ki lr ls lt km lu lv lw kq lx ly lz kt ma mb mc kw md me mf kz mg bi translated">CPU上的推理</h1><p id="e3a7" class="pw-post-body-paragraph iq ir hi is b it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj lg jl jm jn hb bi translated">一旦我们有了模型权重，我们就可以像往常一样加载它，并在另一个设备(如CPU或GPU)上进行预测。我们还希望推理模型接受灵活的输入批量，这可以通过前面的<code class="du jx jy jz ka b">make_model()</code>函数来实现。</p><figure class="li lj lk ll fd ij"><div class="bz dy l di"><div class="mh mi l"/></div></figure><p id="83f5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">您可以看到推理模型现在采用可变的输入样本，</p><pre class="li lj lk ll fd mk ka ml mm aw mn bi"><span id="2941" class="kg kh hi ka b fi mo mp l mq mr">_________________________________________________________________<br/>Layer (type) Output Shape Param # <br/>=================================================================<br/>Input (InputLayer) (None, 500) 0 <br/>_________________________________________________________________<br/>Embedding (Embedding) (None, 500, 128) 1280000 <br/>_________________________________________________________________<br/>LSTM (LSTM) (None, 32) 20608 <br/>_________________________________________________________________<br/>Output (Dense) (None, 1) 33 <br/>=================================================================</span></pre><p id="3cb8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后，您可以在推理模型中使用标准的<code class="du jx jy jz ka b">fit()</code>、<code class="du jx jy jz ka b">evaluate()</code>函数。</p><h1 id="4368" class="lq kh hi bd ki lr ls lt km lu lv lw kq lx ly lz kt ma mb mc kw md me mf kz mg bi translated">结论和进一步阅读</h1><p id="e2ac" class="pw-post-body-paragraph iq ir hi is b it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj lg jl jm jn hb bi translated">这个快速教程向您展示了如何利用Google Colab上的免费云TPU资源更快地训练Keras模型。</p><p id="74d1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae lb" href="https://cloud.google.com/tpu/docs/" rel="noopener ugc nofollow" target="_blank">云TPU文档</a></p><p id="bb33" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae lb" href="https://cloud.google.com/tpu/docs/performance-guide" rel="noopener ugc nofollow" target="_blank">云TPU性能指南</a></p><p id="5327" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae lb" href="https://cloud.google.com/tpu/docs/troubleshooting" rel="noopener ugc nofollow" target="_blank">云TPU故障排除指南</a></p><p id="77c5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae lb" href="https://www.tensorflow.org/performance/xla/" rel="noopener ugc nofollow" target="_blank"> XLA概述</a></p><p id="eb84" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae lb" href="https://twitter.com/intent/tweet?url=https%3A//www.dlology.com/blog/how-to-train-keras-model-x20-times-faster-with-tpu-for-free/&amp;text=How%20to%20train%20Keras%20model%20x20%20times%20faster%20with%20TPU%20for%20free" rel="noopener ugc nofollow" target="_blank">在Twitter上分享</a> <a class="ae lb" href="https://www.facebook.com/sharer/sharer.php?u=https://www.dlology.com/blog/how-to-train-keras-model-x20-times-faster-with-tpu-for-free/" rel="noopener ugc nofollow" target="_blank">在脸书分享</a></p></div><div class="ab cl ms mt gp mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="hb hc hd he hf"><p id="984a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">【www.dlology.com】最初发表于<a class="ae lb" href="https://www.dlology.com/blog/how-to-train-keras-model-x20-times-faster-with-tpu-for-free/" rel="noopener ugc nofollow" target="_blank"><em class="mz"/></a><em class="mz">。</em></p><figure class="li lj lk ll fd ij er es paragraph-image"><a href="https://medium.com/swlh"><div class="er es na"><img src="../Images/308a8d84fb9b2fab43d66c117fcc4bb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YqDjlKFwScoQYQ62DWEdig.png"/></div></a></figure><h2 id="c148" class="kg kh hi bd ki kj kk kl km kn ko kp kq jb kr ks kt jf ku kv kw jj kx ky kz la bi translated">这篇文章发表在<a class="ae lb" href="https://medium.com/swlh" rel="noopener"> The Startup </a>上，这是Medium最大的创业刊物，拥有+ 378，907读者。</h2><h2 id="9478" class="kg kh hi bd ki kj kk kl km kn ko kp kq jb kr ks kt jf ku kv kw jj kx ky kz la bi translated">在此订阅接收<a class="ae lb" href="http://growthsupply.com/the-startup-newsletter/" rel="noopener ugc nofollow" target="_blank">我们的头条新闻</a>。</h2><figure class="li lj lk ll fd ij er es paragraph-image"><a href="https://medium.com/swlh"><div class="er es na"><img src="../Images/b0164736ea17a63403e660de5dedf91a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ouK9XR4xuNWtCes-TIUNAw.png"/></div></a></figure></div></div>    
</body>
</html>
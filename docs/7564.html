<html>
<head>
<title>Deep Deterministic Policy Gradient</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度确定性政策梯度</h1>
<blockquote>原文：<a href="https://medium.com/swlh/policy-gradients-1edbbbc8de6b?source=collection_archive---------5-----------------------#2018-11-19">https://medium.com/swlh/policy-gradients-1edbbbc8de6b?source=collection_archive---------5-----------------------#2018-11-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/54fc589b6d2d8616656e54d09e6981ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*GHKECSiZEFkKQ6y1IDMFyQ.gif"/></div><figcaption class="im in et er es io ip bd b be z dx">Trained Agent Performance (animation by author)</figcaption></figure><h1 id="eb9a" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">介绍</h1><p id="6700" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">本文概述了我在Reacher环境下对Udacity深度强化学习Nanodegree的第二个项目的实现。在这个项目中，目标是训练一个有两个关节的acrobat手臂，使它能够跟踪一个气球。随着气球的移动，两个关节被调整以跟踪气球。所以这是一个经典的机器人问题。但是对于这个项目，我们将改为使用无模型强化学习来学习最优策略。具体而言，使用的方法是深度确定性政策梯度法(DDPG)。</p><p id="4060" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">基于值的强化学习算法，如DQN，已经在许多领域表现出良好的性能。然而，它们仍然限于离散动作空间环境和确定性策略(因为它们本质上基于确定性贪婪策略，因为它们仅选择均匀随机动作)。此外，使用基于价值的方法，我们首先计算每个状态的价值函数，并使用它来确定最佳策略。这是一种寻找最优策略的间接方法。</p><p id="00c4" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">另一方面，使用基于策略的方法，我们直接找到产生最多回报的策略。策略梯度是一种更有效的基于策略的学习算法，其中我们直接计算预期回报相对于策略参数的梯度。除了直接之外，它还能很好地处理连续动作和随机策略。其他基于策略的方法包括随机优化方法，如随机打靶、交叉熵方法等。</p><p id="a1c0" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在深入研究这个项目的实现之前，我在下面介绍了一些关于策略梯度方法的基础知识。特别是，本文将带领读者从强化学习(RL)的基本目标到一些高级策略梯度算法，如强化、行动者-批评家、优势行动者-批评家、确定性策略梯度和深度确定性策略梯度。为了彻底理解，假设读者精通概率统计、线性代数、向量微积分和基本强化学习术语。</p></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><h1 id="ff35" class="iq ir hi bd is it ky iv iw ix kz iz ja jb la jd je jf lb jh ji jj lc jl jm jn bi translated">强化算法</h1><p id="30dc" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">请注意，在以下分析中，为简单起见，贴现因子γ假定为1。但是所有的分析都可以很容易地推广到γ不为1的情况。</p><p id="c741" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">所有强化学习(RL)的基本目标是最大化期望总效用Uθ，其定义如下[1]:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es ld"><img src="../Images/1eff9a5491251b94f8024100710496b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*0HiGaa6EflDLVaoIxY-A-w.png"/></div></figure><p id="8ae9" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">做了一些数学计算后，可以看出Uθ等于Q(s₀,a₀).的期望值如果初始状态分布是均匀的，那么这意味着RL的目标是找到一个使所有可能状态的q值最大化的策略。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es li"><img src="../Images/9b4772baea9c0cd13a86735f0e9ca868.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*PobCVwl3PPDWVGc0fK2MPA.png"/></div></figure><p id="1033" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">使用期望值的定义，上述等式1a可以重写为:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es lj"><img src="../Images/8bbb9a29818f55535273c427b6372ecd.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*2VUfUTaUejgTWF66QkS9Qg.png"/></div></figure><p id="a484" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">使用策略梯度法，我们可以通过首先计算Uθ相对于θ的梯度来最大化uθ，其(使用加强对数似然技巧[3])可以推导为:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es lk"><img src="../Images/00df8cf808480413b6781052b8d49710.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*wLNBZTfqMgME5L4lQiOHNQ.png"/></div></figure><p id="a1ca" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">提高预期总回报的一种方法是在当前θ中随机添加噪声，如果它导致更好的总回报，那么我们保留它，否则我们忽略它，并不断重复这一过程。这种方法叫做随机放炮法。同样还有其他更复杂的方法，如交叉熵方法。所有这些方法都属于随机优化算法的范畴。然而，尽管这些方法实现起来非常简单，但是它们效率不高，并且不能很好地适应高维空间。更有效的方法是使用随机梯度上升沿梯度方向改变θ，如下所示:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/f55f8fc86d5381674c9a1448af950630.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*7wRHAwdeYtkkk4jBqs62-g.png"/></div></figure><p id="d749" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">利用上述梯度的基本策略梯度算法被称为加强算法，其工作原理如下:</p><h2 id="4a7e" class="lm ir hi bd is ln lo lp iw lq lr ls ja jz lt lu je kd lv lw ji kh lx ly jm lz bi translated">一种基本强化算法:</h2><p id="c6b4" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">从随机向量θ开始，重复以下3个步骤，直到收敛:</p><p id="d06c" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">1.使用策略Pθ(at|st)收集m个轨迹{τ1，τ2，…，τm}，其中每个轨迹如上所定义。</p><p id="9aa1" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">2.使用这些轨迹计算梯度的蒙特卡罗估计值，如下所示:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es ma"><img src="../Images/9418da2e9b5a32deb1c6f40220d3f7d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*WxTfeJyZ0Ujxg6WKFx0gMQ.png"/></div></figure><p id="2c12" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">请注意，上述估计器有效的原因是因为轨迹是通过遵循正在学习的策略生成的，即Pθ(τ)，即它是一种基于策略的算法。另一种说法是，我们从概率分布Pθ(τ)中对{τ1，τ2，…，τm}中的每个轨迹进行采样。</p><p id="0018" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">3.使用上述梯度估计器更新策略网络的权重/参数:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/397ba336622ad7067a0deb0698d8deda.png" data-original-src="https://miro.medium.com/v2/resize:fit:390/format:webp/1*9nEeXTz5fsXGkLKeDoIUwQ.png"/></div></figure><p id="1162" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">强化算法背后的直觉是，如果总回报是正的，那么在该轨迹中采取的所有行动都被强化，而如果总回报是负的，那么在该轨迹中采取的所有行动都被抑制。此外，为了提高计算效率，通常将m设置为1。</p><p id="718c" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">虽然比随机优化方法更好，但加强算法也有一些缺点。梯度估计是相当嘈杂的，特别是对于m=1的情况，因为一个单一的轨迹可能不代表政策。<br/> 2。没有明确的学分分配。一个轨迹可能包含许多好的和坏的行为，这些行为是否得到强化，只取决于从初始状态开始所获得的总回报。<br/> 3。它对奖励的绝对值非常敏感。例如，给所有的奖励加上一个固定的常数可以极大地改变算法的行为。这样一个微不足道的转换应该不会对最优策略产生影响。</p><p id="ad49" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">根据梯度的定义，∇θUθ指向Uθ变化最大的方向。然而，从根本上讲，增强算法的上述缺点是由于∇θUθ(即ĝ)的蒙特卡罗估计量具有高方差的事实。如果我们能减少它的方差，那么我们对梯度(ĝ)的估计将更接近真实的梯度∇θUθ.</p><p id="1e47" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">虽然梯度的蒙特卡罗估计(ĝ)是无偏的，但它表现出很高的方差。如下所述，有几种方法可以在不引入偏倚的情况下减少方差:1)使用因果关系，2)使用基线。</p><h1 id="1d00" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">演员-评论家算法</h1><p id="5ad4" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">减少差异的一个方法是利用因果关系:ĝ根据总报酬而不是未来的报酬来更新轨迹中的所有行动。也就是说，未来的行为影响过去的回报，这在我们的因果宇宙中是不可能的。因此，我们可以通过使用奖励来使梯度估计值更加真实，如下式所示。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/cffa56a4418918a87ce00919f5cc2aa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*i5bgoLVwc2lPztZvHwlZig.png"/></div></figure><p id="fdb7" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">请注意，使用奖励去代替总奖励仍然导致∇θUθ的无偏估计，因为因果关系是在等式3中使用Pθ(τ)的期望中处理的。此外，这样做减少了差异，因为奖励去表达式有更少的条款(因此不确定性更低)比总奖励表达式。</p><p id="fd2a" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">需要注意的一点是，要获得的回报实际上是对(st，at)的q值的估计。这是因为q值定义如下:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es md"><img src="../Images/d5dd11ac45b04b12907e23ca87140622.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*9ycK9rPgu2-p83Mm_kQRpw.png"/></div></figure><p id="6343" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">因此，如果轨迹τ是从Pθ(τ)采样的，那么QPθ(st，at)的单样本蒙特卡罗估计就是:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es me"><img src="../Images/1328d4edaa939b2d0d91400570eb9c30.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*ALqTZRqpJunqJ6mOZmGK3w.png"/></div></figure><p id="3d0e" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">如上所示，我们可以使用奖励go的Q值估计量，而不是像等式7那样使用奖励go的蒙特卡罗估计量。因此，等式7可以重写为:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/6a843a9d9755ec47b0629470be6bea55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*5ZZlaBYoI9lWnQTj4FxBzA.png"/></div></figure><p id="6e38" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">如果Qhat Pθ(st，at)使用神经网络(由w参数化)建模，则我们得到:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/81102278f3bc7dd992de7a845060bf9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*KWIMPDccA4Xut42Q3kzHYw.png"/></div></figure><p id="08cc" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">注意，因为状态-动作空间可以是非常高维的，所以它很快就遇到了贝尔曼的维数灾难；因此，在大多数具有复杂状态转换动态的实际情况下，Qhat Pθ(st，at)使用基于神经网络的函数逼近器来建模。</p><p id="3a0d" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">那么等式10可以重写为:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/dafcca57afe8997ebb4658b49925680d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*h0y1VuWlqyOjqkgFdJtFog.png"/></div></figure><p id="e430" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">其中，Pθ(at | st)是由θ参数化的演员网络，而Qhat Pθ(st，at)是由w参数化的评论家网络，这实质上就是所谓的演员-评论家算法。</p><p id="5002" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">对于任何被访问的状态-动作对(s，a ),行动者网络使用等式6来更新(利用来自等式12的ĝ),而批评者网络通常使用时间差学习来更新(由于其比蒙特卡罗学习更低的方差),使用以下更新等式:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/47d7631efd54faef4ffda49176bb3fca.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*zZPN2aNtWa_87FxlA3O75w.png"/></div></figure><p id="f756" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">由此更新权重向量w以减少损失L(w ),损失L(w)被定义为:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/3d9a457c3c736aa8ff9eab4897780a11.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*123C6nLUD_loxcUPcV8-4g.png"/></div></figure><p id="3bac" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">并且使用Q-learning(以便批评家基于偏离策略的算法):</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mk"><img src="../Images/6bfe571593d4924555fca4290ffb33c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*Wt7WB2aiemW5OS2_xInaJQ.png"/></div></figure><p id="efcb" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">因此</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es ml"><img src="../Images/d305bde2382481f08c75dd34372496c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*S_zE9RwtuwbqCL0cr0Mlxw.png"/></div></figure><p id="aa2e" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">凭借</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mm"><img src="../Images/57cbad1b06e00a15669f24d145d48e99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*UU2hJyoE08CXwDY-Soy_Bw.png"/></div></figure><p id="f4c8" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这是演员-评论家算法的基础。虽然它有许多变体，但正如我们将在下面看到的，这是它的基本核心。</p><h1 id="58e2" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">优势行动者-评论家算法</h1><p id="666f" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">除了使用奖励去(由于因果关系)，另一种最小化ĝ方差的方法是减去不依赖于θ或行动a的基线b——这个组合项被称为优势函数。可以从数学上证明，这样的变换不仅无偏，而且减少了方差。为什么它减少方差的直观解释是因为乘以∇θlog(Pθ(a|s的项具有较小的量级，这实质上减少了整个表达式的方差。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/e51f27d271ee812672735f3111a0c076.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*iadh22CZRtpgdwyVdvwmRQ.png"/></div></figure><p id="878b" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">基线b有多种选择，理论上也可以计算出b的最佳值。然而，为了简单和直观起见，通常使用的基线是所有动作的平均q值，即状态值。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mo"><img src="../Images/cb36245b1982c627845709ab4a3a9d4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*weBVmGHhdis6ebbyt8P1hw.png"/></div></figure><p id="6012" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">然后，优势函数被写成如下:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mp"><img src="../Images/30880bbdd606c77b83ae133ae60096f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*F0XViPuYXNHBq6QCFZf63g.png"/></div></figure><p id="f338" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">使用该优势函数的基本思想是，q值高于平均值(即状态值)的动作被加强，而其他动作被抑制。这比原始增强算法中使用的梯度方程更直观。所以从数学上来说，它会导致更低的方差，这一点也不奇怪。此外，现在梯度不再取决于奖励的绝对值。</p><p id="c68b" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">上述等式的一个问题是，在实践中，很难计算上述期望——特别是对于连续动作或高维动作空间。因此，状态-值函数用单独的神经网络建模，该神经网络由wᵥ参数化如下:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mq"><img src="../Images/dd2c9d8c629c22edab5047cf2b511965.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*ISIH328-hlS4NgW7TuuvCA.png"/></div></figure><p id="30e6" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">优势函数现在变为:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es lk"><img src="../Images/d7725662483eeaffae08ab8ef7aedd90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*YE89yKxZXTvkCi9feSbM3w.png"/></div></figure><p id="e199" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这个优势函数的问题是它需要两个独立的神经网络。通过一些巧妙的重新排序，我们可以使用单个神经网络重写优势函数。然而，为了做到这一点，让我们首先重新审视上述分析。基本上，我们想要的理想优势函数是:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/d5e770bec5d23706415d226cea857335.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*T1JFCtKYq9QZ5IgG-ijP_Q.png"/></div></figure><p id="c54e" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">如上面等式8中所定义的，状态-动作值可以根据状态-值函数进一步简化为:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es ms"><img src="../Images/862f12f0e78e283435f14754b3cd1216.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*H9d2LlTAhWRQ2GZY5DGbQQ.png"/></div></figure><p id="be9e" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">上式中定义的QPθ(st，at)的单样本蒙特卡罗估计值为:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mt"><img src="../Images/ab92a1377d95e8039bb90ae60cfb77b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*wy4pMbVYoCTQ3VZCKdVYAQ.png"/></div></figure><p id="dcbf" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">因此，现在我们只需要使用由wv参数化的神经网络来表示状态值函数，如下所示:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mu"><img src="../Images/3c99022662fa08080fa3928723982795.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/1*GXQ5evagJ1QMJ8FRXLGlIA.png"/></div></figure><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mv"><img src="../Images/c67df17bf5a0dd473f0ab78de725b2ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*P5mFRMnYbm0HI9DgbMRVuQ.png"/></div></figure><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mw"><img src="../Images/3208cddf78bd6fc30b77837814fed5d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*HvZYPYc2hFp7MNfiqJwcIw.png"/></div></figure><p id="f5c9" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">因此现在可以使用用wᵥ.参数化的单个神经网络来表示优势函数注意，对于上述优势函数等式，它实际上只是一步TD误差(即TD(0)误差)。此外，也可以用TD(λ)误差来表示。</p><p id="a369" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">优势演员评论家的梯度方程现在是:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mx"><img src="../Images/80839febb266fda91e2bdd82cc8b49e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*AZhsm0MQmI7EPfa1eidNcw.png"/></div></figure><p id="0616" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这将是对预期梯度(等式3)的更好的估计，即具有更低的方差，并且仍然是无偏的，即使m=1。这样一来，算法的学习速度会快很多。</p><p id="a699" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">wv更新如下:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es my"><img src="../Images/1f474c48219570c3fd035991e9f579f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*Xm4oHIq2SMuEMFSzy6D-vw.png"/></div></figure><p id="528f" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">由此使用一步TD学习(即TD(0)):</p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="er es mz"><img src="../Images/d7d3b0aedae6a4312e36323af33c9edd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PHs463beu3XzUew9d5uWag.png"/></div></div></figure><p id="794e" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">使用来自等式29的梯度估计器、来自等式30的权重更新以及来自基本增强算法的剩余步骤，得到了所谓的优势因素-评价算法。</p><p id="d45f" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">简要总结上述讨论，增强算法的主要缺点是梯度估计器基于初始状态-行动对的预期总回报的蒙特卡罗估计器，虽然偏差低，但方差高。通过使用因果关系并从蒙特卡罗估计中减去基线，我们可以减少方差。通过使用期望总报酬的TD估计量来代替蒙特卡罗估计量，方差被进一步减小。</p><h1 id="f23c" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">确定性策略梯度(DPG)算法</h1><p id="0d62" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">对于连续环境中的随机策略，参与者输出高斯分布的均值和方差。并且从该高斯分布中采样动作。对于确定性动作，虽然这种方法仍然有效，因为网络将学习具有非常低的方差，但是它涉及复杂性和计算负担，这不必要地减慢了学习算法。为了解决这些缺点，对于确定性行动，我们可以使用所谓的确定性政策梯度。</p><p id="5959" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在随机情况下，策略梯度算法在状态和动作空间上积分，而在确定性情况下，它仅在状态空间上积分。因此，计算确定性策略梯度可能需要更少的样本。但是，为了充分探索状态空间，基本思想是根据随机行为策略选择动作，并了解确定性目标策略(即，需要是偏离策略的算法)。</p><p id="d511" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">DPG本质上是演员-评论家算法的确定性版本。对于基本的DPG算法，我们有两个神经网络，一个网络(由θ参数化)估计最佳目标策略，第二个网络(由w参数化)估计对应于目标策略的动作值函数。下面的等式对此进行了形式化。</p><p id="0751" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">如上所述，因为目标策略是确定性的，所以参与者可能不会很好地探索状态空间来找到最优策略。为了解决这个问题，我们使用一个不同于目标策略的行为策略(b(st))。它基本上是带有一些附加噪声的目标策略。为简单起见，我们将使用正态分布作为噪声源。但是请注意，这个术语就像一个超参数，在下面Reacher环境的实现中，使用了不同的噪声处理。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es ne"><img src="../Images/738158d0a9681497bf24f5f11e093dfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*Yo-I8C4wnAWR5GG2PEQd4w.png"/></div></figure><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es nf"><img src="../Images/bcf8c74aaf624c8dc43542a2e460a2e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*y6fl4yxBIKjOaapfGF3uIQ.png"/></div></figure><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es ng"><img src="../Images/4cdde58b5f6be3a3f182cc28e4107304.png" data-original-src="https://miro.medium.com/v2/resize:fit:388/format:webp/1*mWQYSPYUcQHAx4QpQWyQYA.png"/></div></figure><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es nh"><img src="../Images/5ec027f10955008c3df9d939f5f8efc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*--FDHGgSFnhFu6dkYeLOpw.png"/></div></figure><h2 id="cb3d" class="lm ir hi bd is ln lo lp iw lq lr ls ja jz lt lu je kd lv lw ji kh lx ly jm lz bi translated">确定性策略梯度更新:</h2><p id="2a63" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">1.演员网络更新如下:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es ni"><img src="../Images/f112960ea6d54555a9569fcd096ea503.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*E_PfRyO4_pxIXZ5kytB10Q.png"/></div></figure><p id="999d" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">根据链式法则，它变成了:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es nj"><img src="../Images/958726aeaffa3d50c3390f3570767a58.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*IPiWNeBmV0iyyeTT64SYmA.png"/></div></figure><p id="3200" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">2.评论家网络更新如下:</p><p id="b459" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">TD误差由下式给出:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es nk"><img src="../Images/5e77b360c8ea914b4f256f8574a221ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*aG1m3-N6zMhuotWV6reh0w.png"/></div></figure><p id="d303" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">权重更新为:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es nl"><img src="../Images/128ab2e13b61648cdac9e9833df9f55b.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*6_tlPIAvw_v5o_8o8oQJJg.png"/></div></figure><p id="f92e" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">重申一下，为了恰当地平衡勘探-开发权衡，虽然目标策略μ是确定性的，但行为策略是随机的。所以这是DPG算法的非政策版本。虽然随机策略外行动者-批评者算法通常对行动者和批评者都使用重要性采样，但是因为确定性策略梯度消除了对行动的期望，并且由于目标和行为策略在相同的环境中操作，所以它们的状态转换动态是相同的，所以不需要重要性采样比率。因此，我们可以避免在演员中使用重要性抽样，同样的道理，我们可以避免在评论家中使用重要性抽样[2]。对于那些想知道的人来说，类似的推理适用于为什么我们不在Q-learning中使用重要性抽样。</p><h1 id="d87e" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">深度确定性策略梯度(DDPG)算法</h1><p id="c86a" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">DDPG基本上是DPG，在训练上有一些来自DQN的改变。</p><p id="24c7" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">使用神经网络进行强化学习时的一个挑战是，大多数优化算法都假设样本独立且同分布。显然，这种假设不成立，因为样本是通过在一个环境中顺序探索而生成的。因为DDPG是一个非策略算法，我们可以像在DQN一样使用重放缓冲区(一个有限大小的缓存)来解决这个问题。在每个时间步，演员和评论家通过从缓冲器[2]中均匀地采样一个迷你批次来更新。</p><p id="2927" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">对于批评家来说，由于被更新的网络也用于计算目标，这可能导致像神经网络这样的高度非线性函数逼近器的训练不稳定性。解决这个问题的一个办法是使用一个单独的目标网络，就像DQN [2]一样。给定使用评论家和演员网络确定的目标值，我们创建这两个网络的副本，并将它们的权重软更新到各自的学习网络。详情请参考我的<a class="ae nm" href="https://github.com/amitp-ai/Deep_Reinforcement_Learning/blob/master/P2_Continuous_Actions/Continuous_Control_UdacityWorkspace.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>代码。</p></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><h1 id="9087" class="iq ir hi bd is it ky iv iw ix kz iz ja jb la jd je jf lb jh ji jj lc jl jm jn bi translated">Reacher环境的DDPG实现</h1><p id="2b25" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">现在已经看到了一些常用的策略梯度算法，我们现在可以开始我为Udacity的Reacher项目实现了。在这种环境下，双关节臂(acrobot)可以移动到目标位置(即气球所在的位置)。代理人的手在目标位置的每一步提供+0.1的奖励。因此，代理的目标是在尽可能多的时间步长内保持其在目标位置的位置。随着气球的移动，两个关节被调整以跟踪气球。所以这是一个经典的机器人项目，使用无模型强化学习，代理将学习最优策略。具体而言，使用的方法是深度确定性政策梯度法(DDPG)。观察空间由33个变量组成，对应于手臂的位置、旋转、速度和角速度。每个动作都是一个有四个数字的向量，对应着适用于两个关节的扭矩。动作向量中的每个条目都是介于-1和1之间的数字。</p><p id="4b95" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">使用的Reacher环境包含20个相同的代理，每个代理都有自己的环境副本。为了被认为是解决了，代理必须得到+30的平均分数(超过100个连续的情节，并且超过所有20个代理)。特别是，在每集之后，我们将每个代理人收到的奖励相加(不打折)，得到每个代理人的分数。这产生了20个(可能不同的)分数。然后我们取这20个分数的平均值。<br/>得出每集的平均分(所有20个代理的平均分)。当这些平均分的平均值(超过100集)至少为+30时，环境被视为已解决。</p><p id="0f7d" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">DDPG算法使用4个独立的神经网络。一个学习政策，一个学习价值函数，一个学习目标价值函数，一个学习目标行动-价值函数网络中的目标行动。如前所述，我们使用单独的目标网络而不是本地网络，以防止当TD目标依赖于本地网络时学习中的任何不稳定性。目标网络中的权重更新得非常慢，比如在每个时间步长向本地网络更新0.1%。缓慢地改变目标网络确实会降低学习速度，但它有助于学习算法的稳定性[2]。</p><p id="6fb4" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">为了加速学习过程，由于所有20个代理同时经历，输入数据流相当大。为了充分利用这一点，我在每次迭代中执行4次网络参数更新。不使用重要性抽样比率的原因是，参与者的目标策略是确定的，批评家的目标值也是确定的。此外，评论家网络的梯度修剪为1。这样可以防止评论家网络变化太快。此外，我将目标网络初始化为与正在学习的网络具有相同的(随机)权重。此外，使用奥恩斯坦-乌伦贝克噪声过程代替正态分布来生成行为策略，以便更好地进行探索。所有这些因素加在一起，使得代理在100多集的时间里就达到了学习目标。详情请参考我的<a class="ae nm" href="https://github.com/amitp-ai/Deep_Reinforcement_Learning/blob/master/P2_Continuous_Actions/Continuous_Control_UdacityWorkspace.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>代码。</p><p id="e2f7" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">下面是所有20个代理的平均算法学习性能。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/3837b7082cd0a03fcb7293acecce3476.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*F0xXK_k5Ev2YsIhx0Am_kA.png"/></div><figcaption class="im in et er es io ip bd b be z dx">Training Profile (image by author)</figcaption></figure><p id="3553" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">就使用的超参数而言，演员和评论家网络的学习率都是1e-4，没有使用正则化，因为网络相当小。我逐渐衰减探索概率以获得最优策略。奥恩斯坦-乌伦贝克噪声过程使用平均值0和西格玛值0.2。actor网络是使用三层神经网络(第一层256个神经元，第二层128个神经元，最后输出层4个神经元)构建的。critic网络也是使用三层和与actor网络相似数量的神经元构建的，除了动作被连接到第一层的输出，并且最后一层具有单个输出神经元。为了更快地学习，elu非线性被用于两个网络。</p><p id="b6e0" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">就进一步提高代理性能的方法而言，我的待办事项列表中有几件事情:1)使用优先化经验重放来训练代理，以及2)使用最近的策略优化算法。</p><h2 id="a36b" class="lm ir hi bd is ln lo lp iw lq lr ls ja jz lt lu je kd lv lw ji kh lx ly jm lz bi translated">参考资料:</h2><p id="1109" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">1:加州大学柏克莱分校CS294讲座(【http://rail.eecs.berkeley.edu/deeprlcourse/】T2)<br/>2。https://arxiv.org/pdf/1509.02971.pdf纸(<a class="ae nm" href="https://arxiv.org/pdf/1509.02971.pdf" rel="noopener ugc nofollow" target="_blank"/>)<br/>3。加固绝招:(<a class="ae nm" href="https://dallascard.github.io/the-reinforce-trick.html" rel="noopener ugc nofollow" target="_blank">https://dallascard.github.io/the-reinforce-trick.html</a>)</p></div></div>    
</body>
</html>
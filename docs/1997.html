<html>
<head>
<title>Apache Spark Streaming Simplified</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简化的Apache Spark流</h1>
<blockquote>原文：<a href="https://medium.com/swlh/apache-spark-streaming-simplified-3107f1580b30?source=collection_archive---------3-----------------------#2018-01-11">https://medium.com/swlh/apache-spark-streaming-simplified-3107f1580b30?source=collection_archive---------3-----------------------#2018-01-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/e7c691421323c125f4e21ac14fb4fe43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0qFdLWjc0LQDkRXi7Qt-lw.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">A typical spark streaming data pipeline.</figcaption></figure><p id="642f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">上述数据流描述了用于流数据分析的典型流数据管道。好吧，让我们把它分开，你需要一个源，在这个例子中，我将使用一个分隔文件作为Kafka主题的源。我们可以通过多种方式向卡夫卡发送数据。你可以写一个Kafka生产者或者使用一个像Flume这样的服务，source作为文件，sink作为Kafka。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es js"><img src="../Images/c3c0dcb642ca42c353612659df33e021.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/1*d4NNuHYs1GqsRsL7BBk39Q.jpeg"/></div></figure><p id="2eb8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">下面显示了一个使用java API的示例Kafka生成器，它读取文件并将数据作为消息逐行发送到Kafka中预定义的主题。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="cad7" class="kc kd hi jy b fi ke kf l kg kh">import java.io.BufferedReader;<br/>import java.io.IOException;<br/>import java.io.FileReader;<br/>import java.util.Properties;<br/>import java.util.concurrent.ExecutionException;<br/>import org.apache.kafka.clients.producer.KafkaProducer;<br/>import org.apache.kafka.clients.producer.ProducerRecord;</span><span id="d87b" class="kc kd hi jy b fi ki kf l kg kh">public class KafkaProducerFile {</span><span id="c3f7" class="kc kd hi jy b fi ki kf l kg kh">public static void main(String[] args) throws InterruptedException,<br/>   ExecutionException {<br/>  final String fileName = "/resources/SalesJan.csv";<br/>  String line;<br/>  String topicName = test;<br/>  final KafkaProducer&lt;String, String&gt; kafkaProducer;<br/>  Properties properties = new Properties();<br/>  properties.put("bootstrap.servers", "localhost:9092");<br/>  properties.put("client.id", "KafkaFileProducer");<br/>  properties.put("key.serializer",<br/>    "org.apache.kafka.common.serialization.StringSerializer");<br/>  properties.put("value.serializer",<br/>    "org.apache.kafka.common.serialization.StringSerializer");<br/>  kafkaProducer = new KafkaProducer&lt;String, String&gt;(properties);<br/>  int count = 0;<br/>  try (BufferedReader bufferedReader = new BufferedReader(new FileReader(<br/>    fileName))) {</span><span id="a45a" class="kc kd hi jy b fi ki kf l kg kh">while ((line = bufferedReader.readLine()) != null) {<br/>    count++;<br/>    kafkaProducer.send(new ProducerRecord&lt;String, String&gt;(<br/>      topicName, Integer.toString(count), line));<br/>   }</span><span id="62c7" class="kc kd hi jy b fi ki kf l kg kh">} catch (IOException e) {<br/>   e.printStackTrace();<br/>  }<br/> }</span><span id="36ca" class="kc kd hi jy b fi ki kf l kg kh">}</span></pre><p id="e79b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">因此，现在我们的Kafka主题中有了数据，如果我们打开一个控制台消费程序，它应该会开始将文件内容显示为Kafka消息。</p><p id="f091" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">&gt; bin/Kafka-console-consumer . sh—bootstrap-server localhost:9092—主题测试</p><p id="314f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在，我们将看看如何使用spark流来消费这些数据。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="be26" class="kc kd hi jy b fi ke kf l kg kh">import java.util.Collections;<br/>import java.util.HashMap;<br/>import java.util.Map;<br/>import java.util.Set;<br/>import kafka.serializer.StringDecoder;<br/>import org.apache.spark.SparkConf;<br/>import org.apache.spark.api.java.JavaSparkContext;<br/>import org.apache.spark.api.java.function.Function;<br/>import org.apache.spark.streaming.Duration;<br/>import org.apache.spark.streaming.api.java.JavaDStream;<br/>import org.apache.spark.streaming.api.java.JavaPairInputDStream;<br/>import org.apache.spark.streaming.api.java.JavaStreamingContext;<br/>import org.apache.spark.streaming.kafka.KafkaUtils;<br/>import scala.Tuple2;</span><span id="dd92" class="kc kd hi jy b fi ki kf l kg kh">public class KafkaSparkStream {</span><span id="2698" class="kc kd hi jy b fi ki kf l kg kh">public static void main(String[] args) throws InterruptedException {</span><span id="1f55" class="kc kd hi jy b fi ki kf l kg kh">SparkConf sparkConf = new SparkConf().setAppName("kafkaSparkStream")<br/>    .setMaster("local[*]");<br/>  JavaSparkContext sc = new JavaSparkContext(sparkConf);<br/>  JavaStreamingContext ssc = new JavaStreamingContext(sc, new       <strong class="jy kj">Duration(5000)</strong>);<br/>  Map&lt;String, String&gt; kafkaParams = new HashMap&lt;String, String&gt;();<br/>  kafkaParams.put("bootstrap.servers", "localhost:9092");<br/>  kafkaParams.put("group.id", "1");<br/>  Set&lt;String&gt; topicName = Collections.singleton("test");</span><span id="1458" class="kc kd hi jy b fi ki kf l kg kh">JavaPairInputDStream&lt;String, String&gt; kafkaSparkPairInputDStream = KafkaUtils<br/>    .createDirectStream(ssc, String.class, String.class,<br/>      StringDecoder.class, StringDecoder.class, kafkaParams,<br/>      topicName);</span><span id="cfc5" class="kc kd hi jy b fi ki kf l kg kh">JavaDStream&lt;String&gt; kafkaSparkInputDStream = kafkaSparkPairInputDStream<br/>    .map(new Function&lt;Tuple2&lt;String, String&gt;, String&gt;() {</span><span id="cee1" class="kc kd hi jy b fi ki kf l kg kh">private static final long serialVersionUID = 1L;</span><span id="c364" class="kc kd hi jy b fi ki kf l kg kh">public String call(Tuple2&lt;String, String&gt; tuple2) {<br/>      return tuple2._2();<br/>     }<br/>    });</span><span id="83d6" class="kc kd hi jy b fi ki kf l kg kh"><strong class="jy kj">kafkaSparkInputDStream.print();</strong><br/>  ssc.start();<br/>  ssc.awaitTermination();<br/> }<br/>}</span></pre><p id="1371" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">提交时，作业将连接到Kafka主题，并在指定的时间间隔(<strong class="iw kj">持续时间(5000)) </strong>内累积数据。应该在作业中指定一些火花动作来执行它。<strong class="iw kj">kafkasparkinputdstream . print()</strong>是持续将数据流内容打印到控制台的作业动作。任何ETL都可以使用spark转换和操作或Spark SQL应用于数据，丰富的数据可以推送到API支持的不同目的地。</p><p id="bd59" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我们看看spark streaming是如何处理数据的。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kk"><img src="../Images/6e4cc9079f2bbfec69cf5af10c2f0b51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*WPP0urr4EG2SwaLLuSacMw.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx">Dstreams are split into RDDS and are processed by the executors.</figcaption></figure><p id="874d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">当作业运行时，来自Kafka的每5秒钟的数据被转换成一个数据流。正是在这一点上，spark将它的并行性带入了画面。数据流被分成多个RDD，并被送到执行器进行处理。处理后的数据可以发送到任何支持的目的地，也可以直接写入本地磁盘。spark流作业中数据消耗的并行性取决于Kafka主题中的分区数量，这在内部意味着作业启动的消费者数量将等于分区数量。</p><p id="a0e4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">一些调谐技巧。请注意以下参数。这些应该根据Hadoop集群资源可用性进行调整。</p><p id="b5aa" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">spark.executor.cores=5 —集群中每个执行器使用的内核数量<br/>Spark . driver . memory = 8g—Spark 2需要更好的驱动程序堆大小。这是OOM错误的主要原因。<br/> spark.executor.memory=10g —集群中每个执行器使用的内存<br/>spark . executor . instances = 10—应该使用的执行器数量。这取决于资源的可用性。如果可用的总内存是10 GB，每个执行器有2 GB，那么可能的执行器的最大数量将是3。其余的内存很可能会分配给其他资源，并作为开销保留下来。</p><p id="d0d6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">感谢阅读！！</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kl"><img src="../Images/731acf26f5d44fdc58d99a6388fe935d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6gfnVvkMRFtjVsWF7vkClA.png"/></div></div></figure><h2 id="fd41" class="kc kd hi bd km kn ko kp kq kr ks kt ku jf kv kw kx jj ky kz la jn lb lc ld le bi translated">这篇文章发表在<a class="ae lf" href="https://medium.com/swlh" rel="noopener"> The Startup </a>上，这是Medium最大的创业刊物，拥有283，454+读者。</h2><h2 id="7945" class="kc kd hi bd km kn ko kp kq kr ks kt ku jf kv kw kx jj ky kz la jn lb lc ld le bi translated">在这里订阅接收<a class="ae lf" href="http://growthsupply.com/the-startup-newsletter/" rel="noopener ugc nofollow" target="_blank">我们的头条新闻</a>。</h2><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kl"><img src="../Images/731acf26f5d44fdc58d99a6388fe935d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6gfnVvkMRFtjVsWF7vkClA.png"/></div></div></figure></div></div>    
</body>
</html>
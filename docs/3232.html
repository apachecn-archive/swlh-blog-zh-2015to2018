<html>
<head>
<title>Deep Learning for text made easy with AllenNLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">AllenNLP让文本深度学习变得简单</h1>
<blockquote>原文：<a href="https://medium.com/swlh/deep-learning-for-text-made-easy-with-allennlp-62bc79d41f31?source=collection_archive---------2-----------------------#2018-03-19">https://medium.com/swlh/deep-learning-for-text-made-easy-with-allennlp-62bc79d41f31?source=collection_archive---------2-----------------------#2018-03-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/591c3faa673123fa9d042ce233984ff8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ziGFmm8RMMqB-QCJhdE6mA.jpeg"/></div></div></figure><p id="1497" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一个好的学习过程的关键原则之一是保持学习比你目前的理解稍微多一点的东西。如果主题与你已经知道的太相似，你最终<strong class="is jo">不会取得太大进展</strong>。另一方面，如果主题太难，你会<strong class="is jo">停滞不前，取得很少或没有进展</strong>。</p><p id="47d8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">深度学习涉及许多不同的主题和我们需要学习的东西，所以一个好的策略是开始研究人们已经为我们建立的东西。这就是框架伟大的原因。它们让我们不必太在意如何构建模型的细节，这样我们就可以更专注于<strong class="is jo">我们想要完成的</strong>(而不是专注于如何去做)。</p><p id="ae40" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae jp" href="http://allennlp.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="is jo"> AllenNLP </strong> </a>是一个框架，它使得为自然语言处理建立深度学习模型的任务<strong class="is jo">变得真正令人愉快</strong>。这对我来说是一个惊喜，因为我之前在NLP深度学习方面的所有经历都是某种痛苦。</p><p id="634e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">处理NLP任务需要不同类型的神经网络细胞，因此在开始使用AllenNLP框架之前，让我们快速概述一下这些细胞背后的理论。</p></div><div class="ab cl jq jr gp js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="hb hc hd he hf"><h1 id="f014" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">🙆当简单的神经网络不够时</h1><p id="b021" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated">在简单的作品中，阅读文本的任务是建立在我们以前读过的东西上。比如这句话，如果你之前没有读过，可能就没有意义了。所以创造这些神经网络细胞背后的想法是:</p><blockquote class="la"><p id="1cef" class="lb lc hi hj ld le lf lg lh li lj jn dx translated"><em class="lk">“如果人类利用他们之前读过的东西来理解接下来的内容，也许如果我们在模型中使用这种机制，他们可以更好地理解文本，对吗？”</em></p></blockquote><h2 id="abc6" class="ll jy hi bd jz lm ln lo kd lp lq lr kh jb ls lt kl jf lu lv kp jj lw lx kt ly bi translated">🎥<code class="du lz ma mb mc b"> </code>递归神经网络</h2><p id="225c" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated">为了使用一个考虑时间的网络，我们需要一种表示时间的方法。但是我们怎么做呢？</p><blockquote class="md me mf"><p id="e98b" class="iq ir mg is b it iu iv iw ix iy iz ja mh jc jd je mi jg jh ji mj jk jl jm jn hb bi translated">处理具有时间范围的模式的一个显而易见的方法是通过将模式的序列顺序与模式向量的维度相关联来显式地表示时间。<strong class="is jo">第一个时间事件由模式向量中的第一个元素表示，第二个时间事件由模式向量中的第二个位置表示，依此类推。</strong>——<a class="ae jp" href="http://psych.colorado.edu/~kimlab/Elman1990.pdf" rel="noopener ugc nofollow" target="_blank">杰弗里·埃尔曼</a></p></blockquote><p id="2b1a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">问题是这种方法有几个缺点。例如:</p><blockquote class="md me mf"><p id="c467" class="iq ir mg is b it iu iv iw ix iy iz ja mh jc jd je mi jg jh ji mj jk jl jm jn hb bi translated">[…]【T1]移位寄存器对模式的持续时间施加了严格的限制(因为输入层必须提供可能的最长模式)，此外，还建议所有输入向量长度相同。<strong class="is jo">这些问题在语言等领域特别麻烦，在这些领域中，人们希望对可变长度的模式进行可比较的表示。</strong>对于语音的基本单位(语音片段)和句子来说都是如此。</p></blockquote><p id="464a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae jp" href="https://en.wikipedia.org/wiki/Jeffrey_Elman" rel="noopener ugc nofollow" target="_blank"> Jeffrey L. Elman </a>谈到论文中的其他缺点<a class="ae jp" href="http://psych.colorado.edu/~kimlab/Elman1990.pdf" rel="noopener ugc nofollow" target="_blank">及时发现结构</a>。本文介绍了Elman网络，这是一个添加了一组“上下文单元”的三层网络。</p><p id="3812" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你对神经网络完全陌生，也许阅读我写的另一篇文章是个好主意。但简单地说，神经网络是一种具有被输入激活或不被输入激活的单元的东西。</p><p id="450d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Elman根据Jordan (1986)建议的方法开始了他的工作。乔丹推出<strong class="is jo">循环连接</strong>。</p><blockquote class="md me mf"><p id="4bb6" class="iq ir mg is b it iu iv iw ix iy iz ja mh jc jd je mi jg jh ji mj jk jl jm jn hb bi translated">循环连接允许网络的隐藏单元看到它自己先前的输出，因此随后的行为可以由先前的响应来决定。这些循环连接赋予了网络内存。</p></blockquote><p id="0649" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Elman然后添加<strong class="is jo">上下文单元</strong>。这些上下文单元就像一个时钟，告诉我们什么时候应该“放下”之前的输入。但是怎么做呢？与其他神经网络单元一样，上下文单元也有调整权重的机制。</p><p id="5780" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上下文单元和输入都激活神经网络隐藏单元。<strong class="is jo">当神经网络“学习”时，意味着它具有网络处理的所有输入模式的表示。上下文单元记住先前的内部状态。</strong></p><p id="2e58" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果这些都没有多大意义，也不用担心。想象一下，现在我们有了一个神经网络细胞，它把前一个状态考虑进去，产生下一个状态。</p><blockquote class="la"><p id="e1c9" class="lb lc hi hj ld le lf lg lh li lj jn dx translated">“现在我们有了一个神经网络细胞，它可以考虑前一个状态来产生下一个状态。”</p></blockquote><h2 id="0fb7" class="ll jy hi bd jz lm ln lo kd lp lq lr kh jb ls lt kl jf lu lv kp jj lw lx kt ly bi translated">📹当rnn不够用时:LSTM</h2><p id="a7ad" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated">正如Christopher Ola在这里解释的<a class="ae jp" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank"/>(如果你想了解LSTMs的更多信息，这篇文章很棒)有时我们需要更多的上下文，例如，有时我们<strong class="is jo">需要存储很久以前看到的信息</strong>。</p><blockquote class="md me mf"><p id="735f" class="iq ir mg is b it iu iv iw ix iy iz ja mh jc jd je mi jg jh ji mj jk jl jm jn hb bi translated">考虑尝试预测文本中的最后一个单词“我在法国长大……我说一口流利的法语”最近的信息表明，下一个单词可能是一种语言的名称，但如果我们想缩小语言的范围，我们需要法国的上下文，从更远的地方。<strong class="is jo">相关信息和需要信息的地方之间的差距完全有可能变得非常大</strong> — <a class="ae jp" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">克里斯托弗·奥拉</a></p></blockquote><p id="9513" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">LSTM细胞解决了这个问题。他们是一种特殊的RNN，能够学习长期依赖性。我们将只使用LSTM细胞，而不是构建它们，所以为了我们的目的，你可以认为LSTM细胞是具有不同结构的细胞，能够学习长期依赖性。</p></div><div class="ab cl jq jr gp js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="hb hc hd he hf"><h1 id="dafb" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">✨ <code class="du lz ma mb mc b"> </code>为文本分类建立一个奇特的模型</h1><p id="d7ab" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated">好了，理论到此为止，让我们进入有趣的部分，建立模型。</p><figure class="ml mm mn mo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mk"><img src="../Images/1d6bbf1500df363311f92bef8664a097.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ne7S40FqcH7HTI8lgj5GJQ.png"/></div></div><figcaption class="mp mq et er es mr ms bd b be z dx">The training process</figcaption></figure><p id="c102" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上面的图片向我们展示了我们将如何设置一切。首先，我们<strong class="is jo">获取数据</strong>，然后我们<strong class="is jo">将其编码为模型能够理解的格式</strong>(“令牌”和“内部_文本_编码器”)，然后我们向网络提供这些数据，<strong class="is jo">比较标签并调整权重</strong>。在这个过程的最后，模型准备好进行预测。</p><p id="208c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们终于可以感受到AllenNLP的魔力了！我们将用一个简单的JSON文件指定上图中的所有内容。</p><pre class="ml mm mn mo fd mt mc mu mv aw mw bi"><span id="9173" class="ll jy hi mc b fi mx my l mz na">{<br/>  "dataset_reader": {<br/>    "type": "20newsgroups"<br/>  },<br/>  "train_data_path": "train",<br/>  "test_data_path": "test",<br/>  "evaluate_on_test": true,<br/>  "model": {<br/>    "type": "20newsgroups_classifier",<br/>    "model_text_field_embedder": {<br/>      "tokens": {<br/>        "type": "embedding",<br/>        "pretrained_file": "https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.6B.100d.txt.gz",<br/>        "embedding_dim": 100,<br/>        "trainable": false<br/>      }<br/>    },<br/>    "internal_text_encoder": {<br/>      "type": "lstm",<br/>      "bidirectional": true,<br/>      "input_size": 100,<br/>      "hidden_size": 100,<br/>      "num_layers": 1,<br/>      "dropout": 0.2<br/>    },<br/>    "classifier_feedforward": {<br/>      "input_dim": 200,<br/>      "num_layers": 2,<br/>      "hidden_dims": [200, 100],<br/>      "activations": ["relu", "linear"],<br/>      "dropout": [0.2, 0.0]<br/>    }<br/>  },<br/>  "iterator": {<br/>    "type": "bucket",<br/>    "sorting_keys": [["text", "num_tokens"]],<br/>    "batch_size": 64<br/>  },<br/>  "trainer": {<br/>    "num_epochs": 40,<br/>    "patience": 3,<br/>    "cuda_device": 0,<br/>    "grad_clipping": 5.0,<br/>    "validation_metric": "+accuracy",<br/>    "optimizer": {<br/>      "type": "adagrad"<br/>    }<br/>  }<br/>}</span></pre><p id="7863" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们看看那里发生了什么。</p><h2 id="8d73" class="ll jy hi bd jz lm nb lo kd lp nc lr kh jb nd lt kl jf ne lv kp jj nf lx kt ly bi translated">1—数据输入</h2><p id="6a9a" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated">为了告诉AllenNLP输入数据集以及如何从中读取，我们在JSON文件中设置了<code class="du lz ma mb mc b">'dataset_reader'</code>键。</p><blockquote class="md me mf"><p id="5e49" class="iq ir mg is b it iu iv iw ix iy iz ja mh jc jd je mi jg jh ji mj jk jl jm jn hb bi translated">一个<code class="du lz ma mb mc b">DatasetReader</code>从某个位置读取数据并构造一个<code class="du lz ma mb mc b">Dataset</code>。除了文件路径之外，读取数据所需的所有参数都应该传递给<code class="du lz ma mb mc b">DatasetReader</code> — <a class="ae jp" href="https://allenai.github.io/allennlp-docs/api/allennlp.data.dataset_readers.dataset_reader.html?highlight=dataset_reader#module-allennlp.data.dataset_readers.dataset_reader" rel="noopener ugc nofollow" target="_blank"> AllenNLP文档</a>的构造函数</p></blockquote><p id="142b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们的数据集将是20个新组，稍后我们将定义如何从中读取(在python类中)。首先让我们定义模型的其余部分。</p><h2 id="7509" class="ll jy hi bd jz lm nb lo kd lp nc lr kh jb nd lt kl jf ne lv kp jj nf lx kt ly bi translated">2 —模型</h2><p id="1514" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated">为了指定型号，我们将设置<code class="du lz ma mb mc b">'model'</code>键。<code class="du lz ma mb mc b">'model'</code>键内还有三个参数:</p><p id="aebd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><code class="du lz ma mb mc b">'model_text_field_embedder'</code>、<code class="du lz ma mb mc b">'internal_text_encoder'</code>和<code class="du lz ma mb mc b">'classifier_feedforward'</code></p><p id="4fc7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们先处理第一个，剩下的两个留待以后处理。</p><p id="bf1f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用密钥<code class="du lz ma mb mc b">'model_text_field_embedder'</code>，我们告诉AllenNLP在将数据传递给模型之前应该如何对其进行编码。简而言之，我们希望让我们的数据更“有意义”。其背后的想法是这样的:<strong class="is jo">如果我们能够像比较数字一样比较单词会怎么样？</strong></p><p id="b9c5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果<strong class="is jo"> 5 - 3 + 2 = 4 </strong>为什么不是<strong class="is jo">国王-男人+女人=王后</strong>？</p><p id="c356" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用<strong class="is jo">单词嵌入</strong>我们可以做到这一点。这对模型也有好处，因为现在我们不需要使用很多稀疏数组(有很多零的数组)作为输入。</p><blockquote class="md me mf"><p id="1c33" class="iq ir mg is b it iu iv iw ix iy iz ja mh jc jd je mi jg jh ji mj jk jl jm jn hb bi translated">单词嵌入是自然语言处理(NLP)中一组语言建模和特征学习技术的统称<strong class="is jo">，其中来自词汇表的单词或短语被映射到实数的向量</strong>。从概念上讲，它涉及到一个数学上的<a class="ae jp" href="https://en.wikipedia.org/wiki/Embedding" rel="noopener ugc nofollow" target="_blank">嵌入</a>从一个每个单词一维的空间到一个连续的<a class="ae jp" href="https://en.wikipedia.org/wiki/Vector_space" rel="noopener ugc nofollow" target="_blank">更低维度的向量空间</a>。— <a class="ae jp" href="https://en.wikipedia.org/wiki/Word_embedding" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></blockquote><p id="d3a9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在我们的模型中，我们将使用<a class="ae jp" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> GloVe:单词表示的全局向量</a></p><blockquote class="md me mf"><p id="4f77" class="iq ir mg is b it iu iv iw ix iy iz ja mh jc jd je mi jg jh ji mj jk jl jm jn hb bi translated">GloVe是一种无监督学习算法，用于<strong class="is jo">获取单词的矢量表示</strong>。在来自语料库的聚集的全局单词-单词共现统计上执行训练，并且所得的表示展示了单词向量空间的有趣的线性子结构。——<a class="ae jp" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">手套</a></p></blockquote><p id="2822" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果这些有意义的话，就把Glove想象成一个模型，在这个模型中，我们传递单词并把它们编码成向量。我们将每个嵌入向量的大小设置为100。</p><blockquote class="la"><p id="4c53" class="lb lc hi hj ld le lf lg lh li lj jn dx translated">Glove将单词编码成向量</p></blockquote><p id="1410" class="pw-post-body-paragraph iq ir hi is b it ng iv iw ix nh iz ja jb ni jd je jf nj jh ji jj nk jl jm jn hb bi translated">这就是<code class="du lz ma mb mc b">'model_text_field_embedder'</code>的作用。</p><h2 id="5fe3" class="ll jy hi bd jz lm nb lo kd lp nc lr kh jb nd lt kl jf ne lv kp jj nf lx kt ly bi translated">3 —数据迭代器</h2><p id="3469" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated">像往常一样，我们将分批分离训练数据。AllenNLP提供了一个名为<a class="ae jp" href="https://allenai.github.io/allennlp-docs/api/allennlp.data.iterators.html#bucket-iterator" rel="noopener ugc nofollow" target="_blank"> BucketIterator </a>的迭代器，它通过根据每批的最大输入长度填充批来提高计算(填充)效率。为此，它根据每个文本中的标记数量对实例进行排序。我们在<code class="du lz ma mb mc b">'iterator'</code>键中设置这些参数。</p><h2 id="e04b" class="ll jy hi bd jz lm nb lo kd lp nc lr kh jb nd lt kl jf ne lv kp jj nf lx kt ly bi translated">4 —培训师</h2><p id="17ae" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated">最后一步是为培训阶段设置配置。训练器使用AdaGrad optimizer达10个时期，如果在最后3个时期验证准确度没有提高，则停止使用。</p><p id="e635" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了训练模型，我们只需要运行:</p><p id="95b6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><code class="du lz ma mb mc b">python run.py our_classifier.json -s /tmp/your_output_dir_here</code></p><p id="e597" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">另一件很酷的事情是，有了这个框架，我们可以在以后停止和恢复训练。但在此之前，我们需要指定dataset_reader和模型python类。</p></div><div class="ab cl jq jr gp js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="hb hc hd he hf"><h1 id="c252" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">👩🏾‍💻编写AllenNLP Python类</h1><h2 id="f1ef" class="ll jy hi bd jz lm nb lo kd lp nc lr kh jb nd lt kl jf ne lv kp jj nf lx kt ly bi translated">数据集_阅读器. py</h2><p id="b105" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated">我们将使用由<a class="ae jp" href="http://scikit-learn.org/stable/datasets/twenty_newsgroups.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>提供的20个新闻组。要在JSON文件中引用DatasetReader，我们需要注册它:</p><pre class="ml mm mn mo fd mt mc mu mv aw mw bi"><span id="4533" class="ll jy hi mc b fi mx my l mz na">@DatasetReader.register("20newsgroups")<br/>class NewsgroupsDatasetReader(DatasetReader):</span></pre><p id="8247" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">您将实现三个方法:<code class="du lz ma mb mc b">read()</code>和<code class="du lz ma mb mc b">text_to_instance()</code>。</p><ul class=""><li id="eaa3" class="nl nm hi is b it iu ix iy jb nn jf no jj np jn nq nr ns nt bi translated"><code class="du lz ma mb mc b">read()</code></li></ul><p id="4d73" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><code class="du lz ma mb mc b">read()</code>方法从scikit-learn获取数据。使用AllenNLP，您可以设置数据文件的路径(例如JSON文件的路径)，但是在我们的例子中，我们只是像python模块一样导入数据。我们将从数据集中读取每个文本和每个标签，并用<code class="du lz ma mb mc b">text_to_instance()</code>包装它。</p><ul class=""><li id="7cd8" class="nl nm hi is b it iu ix iy jb nn jf no jj np jn nq nr ns nt bi translated"><code class="du lz ma mb mc b">text_to_instance()</code></li></ul><p id="58d9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">该方法"<em class="mg">执行从文本输入到</em> <code class="du lz ma mb mc b"><em class="mg">Instance</em></code> " <a class="ae jp" href="https://allenai.github.io/allennlp-docs/api/allennlp.data.dataset_readers.dataset_reader.html?highlight=datasetreader" rel="noopener ugc nofollow" target="_blank"> (AllenNLP文档</a>)所需的任何标记化或处理。在我们的例子中意味着这样做:</p><pre class="ml mm mn mo fd mt mc mu mv aw mw bi"><span id="d8f1" class="ll jy hi mc b fi mx my l mz na">    @overrides<br/>    def text_to_instance(self, newsgroups_post: str, label: str = None) -&gt; Instance:  <br/>        tokenized_text = self._tokenizer.tokenize(newsgroups_post)<br/>        post_field = <strong class="mc jo">TextField(tokenized_text, self._token_indexers)</strong><br/>        fields = {'post': post_field}<br/>        if label is not None:<br/>            fields['label'] = <strong class="mc jo">LabelField(int(label),skip_indexing=True)</strong><br/>        return Instance(fields)</span></pre><p id="991a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将20个新闻组中的文本和标签包装成<code class="du lz ma mb mc b"><a class="ae jp" href="https://allenai.github.io/allennlp-docs/api/allennlp.data.fields.html?highlight=textfield#text-field" rel="noopener ugc nofollow" target="_blank">TextField</a></code>和<code class="du lz ma mb mc b"><a class="ae jp" href="https://allenai.github.io/allennlp-docs/api/allennlp.data.fields.html?highlight=textfield#allennlp.data.fields.label_field.LabelField" rel="noopener ugc nofollow" target="_blank">LabelField</a></code>。</p><h2 id="5a7d" class="ll jy hi bd jz lm nb lo kd lp nc lr kh jb nd lt kl jf ne lv kp jj nf lx kt ly bi translated">model.py</h2><p id="cc18" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated">我们将使用一个双向LSTM网络，它是一个复制第一循环层的单元。一层按原样接收输入，另一层接收输入序列的反向副本。因此，BLSTM网络被设计为捕获序列数据集的信息并保持来自过去和未来的上下文特征。(来源:<a class="ae jp" href="https://arxiv.org/pdf/1602.04874.pdf" rel="noopener ugc nofollow" target="_blank">双向LSTM递归神经网络中文分词</a>)</p><p id="ba30" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先让我们定义模型<strong class="is jo">的类参数</strong></p><ul class=""><li id="e510" class="nl nm hi is b it iu ix iy jb nn jf no jj np jn nq nr ns nt bi translated"><code class="du lz ma mb mc b">vocab</code></li></ul><blockquote class="md me mf"><p id="2b0b" class="iq ir mg is b it iu iv iw ix iy iz ja mh jc jd je mi jg jh ji mj jk jl jm jn hb bi translated">因为在您的模型中经常有几个不同的映射，所以<code class="du lz ma mb mc b">Vocabulary</code>会跟踪不同的名称空间。在这种情况下，我们有一个文本的“令牌”词汇表(代码中没有显示，但这是在后台使用的默认值)，和一个我们试图预测的标签的“标签”词汇表— <a class="ae jp" href="https://github.com/allenai/allennlp/blob/master/tutorials/getting_started/using_in_your_repo.md" rel="noopener ugc nofollow" target="_blank">在您的项目中使用AllenNLP</a></p></blockquote><ul class=""><li id="03b4" class="nl nm hi is b it iu ix iy jb nn jf no jj np jn nq nr ns nt bi translated"><code class="du lz ma mb mc b">model_text_field_embedder</code></li></ul><p id="8ce6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">用于嵌入我们作为输入获得的令牌<code class="du lz ma mb mc b">TextField</code>。返回单词的手套向量表示。</p><ul class=""><li id="9ba0" class="nl nm hi is b it iu ix iy jb nn jf no jj np jn nq nr ns nt bi translated"><code class="du lz ma mb mc b">internal_text_encoder</code></li></ul><p id="0305" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将使用编码器将输入文本转换为单个向量(RNNs，还记得吗？).<code class="du lz ma mb mc b">Seq2VecEncoder</code>是一个模块，它将一系列向量作为输入，并返回一个向量。</p><ul class=""><li id="078f" class="nl nm hi is b it iu ix iy jb nn jf no jj np jn nq nr ns nt bi translated"><code class="du lz ma mb mc b">num_classes</code> —要预测的标签数量</li></ul><p id="bc76" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我们实现模型类<strong class="is jo">方法</strong></p><ul class=""><li id="459e" class="nl nm hi is b it iu ix iy jb nn jf no jj np jn nq nr ns nt bi translated"><code class="du lz ma mb mc b">forward()</code></li></ul><figure class="ml mm mn mo fd ij er es paragraph-image"><div class="er es nu"><img src="../Images/f6477b26fb45fce509ddeb032583462b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*uX1ES-QYujXNLlEL7cRVIQ.png"/></div><figcaption class="mp mq et er es mr ms bd b be z dx">What the forward() method does</figcaption></figure><blockquote class="md me mf"><p id="5922" class="iq ir mg is b it iu iv iw ix iy iz ja mh jc jd je mi jg jh ji mj jk jl jm jn hb bi translated">该模型做的第一件事是嵌入文本，然后将其编码为一个向量。为了对文本进行编码，我们需要获得掩码，这些掩码表示标记序列中的哪些元素只是为了填充而存在的。</p><p id="a371" class="iq ir mg is b it iu iv iw ix iy iz ja mh jc jd je mi jg jh ji mj jk jl jm jn hb bi translated">然后我们通过一个前馈网络来得到类逻辑。我们将logits通过softmax来获得预测概率。最后，如果给我们一个标签，我们可以计算损失并评估我们的指标。— <a class="ae jp" href="https://github.com/allenai/allennlp/blob/master/tutorials/getting_started/using_in_your_repo.md" rel="noopener ugc nofollow" target="_blank">在您的项目中使用AllenNLP</a></p></blockquote><p id="1955" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正向方法基本上完成模型训练任务。如果你想了解更多一点正在发生的事情，你可以去<a class="ae jp" href="https://medium.freecodecamp.org/big-picture-machine-learning-classifying-text-with-neural-networks-and-tensorflow-d94036ac2274#20cc" rel="noopener ugc nofollow" target="_blank">这里</a>。现在我们来谈谈模型类参数的一个重要部分:<strong class="is jo">分类器_前馈</strong>。</p><blockquote class="md me mf"><p id="0e93" class="iq ir mg is b it iu iv iw ix iy iz ja mh jc jd je mi jg jh ji mj jk jl jm jn hb bi translated">我们需要<code class="du lz ma mb mc b"><em class="hi">Model.forward</em></code>来计算它自己的损失。训练代码将在由<code class="du lz ma mb mc b"><em class="hi">forward</em></code>返回的字典中寻找<code class="du lz ma mb mc b"><em class="hi">loss</em></code>值，并计算该损失的梯度以更新模型的参数。— <a class="ae jp" href="https://github.com/allenai/allennlp/blob/master/tutorials/getting_started/using_in_your_repo.md" rel="noopener ugc nofollow" target="_blank">在您的项目中使用AllenNLP</a></p></blockquote><ul class=""><li id="e433" class="nl nm hi is b it iu ix iy jb nn jf no jj np jn nq nr ns nt bi translated"><code class="du lz ma mb mc b">decode()</code></li></ul><blockquote class="md me mf"><p id="21b7" class="iq ir mg is b it iu iv iw ix iy iz ja mh jc jd je mi jg jh ji mj jk jl jm jn hb bi translated"><code class="du lz ma mb mc b">decode</code>有两个功能:它接受<code class="du lz ma mb mc b">forward</code>的输出，并对其进行任何必要的推断或解码，它将整数转换成字符串，使其可读(例如，用于演示)。— <a class="ae jp" href="https://github.com/allenai/allennlp/blob/master/tutorials/getting_started/using_in_your_repo.md" rel="noopener ugc nofollow" target="_blank">在您的</a>中使用AllenNLP</p></blockquote></div><div class="ab cl jq jr gp js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="hb hc hd he hf"><h1 id="9f72" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">🔃运行代码</h1><p id="1d88" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated">如我之前所说，通过命令行训练模型，我们可以使用:</p><p id="91df" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><code class="du lz ma mb mc b">python run.py our_classifier.json -s /tmp/your_output_dir_here</code></p><p id="5916" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我还创建了一个笔记本，这样我们就可以在Google Colaboratory上运行实验并免费使用GPU，下面是链接:<a class="ae jp" href="https://colab.research.google.com/drive/1q3b5HAkcjYsVd6yhrwnxL2ByqGK08jhQ" rel="noopener ugc nofollow" target="_blank">https://colab . research . Google . com/drive/1 q3b 5 hackcjysvd 6 yhrwnx L2 byqgk 08 jhq</a></p><p id="0dc7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你也可以在<a class="ae jp" href="https://github.com/dmesquita/easy-deep-learning-with-AllenNLP" rel="noopener ugc nofollow" target="_blank">这个库</a>上查看代码。</p><p id="b2f9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们建立了一个简单的分类模型，但可能性是无限的。该框架是一个很好的工具，可以为我们的产品创建有趣的深度学习模型。✨</p></div><div class="ab cl jq jr gp js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="hb hc hd he hf"><figure class="ml mm mn mo fd ij er es paragraph-image"><a href="https://medium.com/swlh"><div class="er es nv"><img src="../Images/308a8d84fb9b2fab43d66c117fcc4bb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YqDjlKFwScoQYQ62DWEdig.png"/></div></a></figure><h2 id="48f8" class="ll jy hi bd jz lm nb lo kd lp nc lr kh jb nd lt kl jf ne lv kp jj nf lx kt ly bi translated">这篇文章发表在<a class="ae jp" href="https://medium.com/swlh" rel="noopener"> The Startup </a>上，这是Medium最大的创业刊物，拥有307，871+读者。</h2><h2 id="24b8" class="ll jy hi bd jz lm nb lo kd lp nc lr kh jb nd lt kl jf ne lv kp jj nf lx kt ly bi translated">订阅接收<a class="ae jp" href="http://growthsupply.com/the-startup-newsletter/" rel="noopener ugc nofollow" target="_blank">我们的头条新闻在这里</a>。</h2><figure class="ml mm mn mo fd ij er es paragraph-image"><a href="https://medium.com/swlh"><div class="er es nv"><img src="../Images/b0164736ea17a63403e660de5dedf91a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ouK9XR4xuNWtCes-TIUNAw.png"/></div></a></figure></div></div>    
</body>
</html>
<html>
<head>
<title>Only Numpy: Deriving Forward feed and Back Propagation in Synthetic Gradient (Decoupled Neural Interfaces) with Interactive Code feat. iamtrask</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">唯一Numpy:用交互式代码专长导出合成梯度中的前馈和反向传播(解耦神经接口)。亚姆特拉斯克</h1>
<blockquote>原文：<a href="https://medium.com/swlh/only-numpy-deriving-forward-feed-and-back-propagation-in-synthetic-gradient-decoupled-neural-ca4c99666bbf?source=collection_archive---------9-----------------------#2017-12-24">https://medium.com/swlh/only-numpy-deriving-forward-feed-and-back-propagation-in-synthetic-gradient-decoupled-neural-ca4c99666bbf?source=collection_archive---------9-----------------------#2017-12-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="5ef6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以第一件事！平安夜快乐，节日快乐。我希望每个人都有一个愉快的假期。不管怎样，让我们开始吧，在阅读之前，我想让你们知道两件非常重要的事情。</p><ol class=""><li id="fbe9" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated"><strong class="ih jm">我还没看完</strong> <a class="ae jn" href="https://arxiv.org/abs/1608.05343" rel="noopener ugc nofollow" target="_blank"> <strong class="ih jm">论文</strong> </a> <strong class="ih jm">(使用合成渐变的解耦神经接口)！我想先讨论一下实现，然后转到论文，我将很快阅读论文，并很快做出自己的实现！！</strong></li><li id="88f0" class="jd je hi ih b ii jo im jp iq jq iu jr iy js jc ji jj jk jl bi translated"><strong class="ih jm">本帖所有的数学都是基于iamtrask对合成渐变的实现请点击链接</strong> <a class="ae jn" href="https://iamtrask.github.io/2017/03/21/synthetic-gradients/" rel="noopener ugc nofollow" target="_blank"> <strong class="ih jm">此处</strong> </a> <strong class="ih jm">阅读神奇教程。</strong></li></ol><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jt"><img src="../Images/520ea0e3f0e2b07de6c0de0f8ce2c907.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wvR-iwZ7JylxinwoNBajSw.jpeg"/></div></div></figure><p id="55a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">更新:忘了加LR表示学习率</p><p id="8998" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们开始一些符号之前，我想澄清一下，我们的成本函数是你们在上面看到的等式。另外，现在<strong class="ih jm"> <em class="kf">忽略右上角的数字，但是它们很快就会变得非常重要</em> </strong>！！！！现在是网络架构。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jt"><img src="../Images/f424a4ba03b441a5491142f6013ed5e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hm9bOe8x-7i9OB-Gqwe9aQ.jpeg"/></div></div></figure><p id="7b01" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们只有三层L1，L2，L3，和三个合成梯度发生器L1SGG，L2SGG，L3SGG(每层下面的菱形)。输入层、层1、层2、层3的尺寸也分别为[1000，24]、[1000，128]、[1000，64]、[1000，12]。现在让我们看看每个权重和合成梯度的尺寸。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es kg"><img src="../Images/f7ba7837ec2d47345a45312c07f9293f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hj7oiqtUDx-oRLW8dLGySQ.jpeg"/></div></div></figure><p id="df02" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">就是这样！再次为审查WkSG -&gt;层K合成梯度。还有，最后要注意的是，我写点积和矩阵转置的方式可能会让一些人困惑。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jt"><img src="../Images/818637b85323ad94cdf49a508dd5aa3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DVDyW1jdNu9zQgd3ZjWNQg.jpeg"/></div></div></figure><p id="b17c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">记数法<strong class="ih jm">中间圈点是点积记数法</strong>，那么上面的等式就是Layer1 = logistic函数(dot_product(x，w1))。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jt"><img src="../Images/fe266958d72a4daf617396082fec46f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3IW5ixj49u0kow6rqzUBvw.jpeg"/></div></div></figure><p id="fecc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">右下方的符号<strong class="ih jm">圆圈是矩阵转置</strong>，所以上面的等式是点积(L1WSG，W1 _转置)</p><p id="ba03" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们执行前馈操作和反向传播！！</p></div><div class="ab cl kh ki gp kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="hb hc hd he hf"><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es ko"><img src="../Images/ce28b4ea29bb7abcf772d28ce038cb54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qZ2tkY9oWaHvSK51P7GlkA.png"/></div></div></figure><p id="6968" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以上是iamtrask的合成渐变的实现(再次链接<a class="ae jn" href="https://iamtrask.github.io/2017/03/21/synthetic-gradients/" rel="noopener ugc nofollow" target="_blank">这里</a>),因为每一层有两个部分。我们将分别研究每个操作。我将描述他们正在做什么，并从第1层开始。</p><p id="de88" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih jm"> <em class="kf">正向和合成更新</em>:执行常规的前馈操作，并使用合成梯度更新当前重量</strong></p><p id="b333" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih jm"> <em class="kf">更新合成权重</em>:使用真实渐变更新合成渐变</strong></p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jt"><img src="../Images/d7d78c324d0155027e515398a2218b57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HoDbZIuebNhYO4hyHBqOfA.jpeg"/></div></div></figure><p id="ddd0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以，这里发生了很多事情。首先，方框中的蓝色数字描述了所得方程的维数。第二…</p><p id="4528" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih jm">T5【C1】-&gt;前进并合成更新<br/> C2 - &gt;更新合成权重-</strong></p><p id="957f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基本上，在C1步骤中，我们执行常规的前馈操作，以获得每一层的输出。例如，Layer1 = log(dot_product(x，w1))，并且在前向馈送操作之后，我们通过使用W1SG(合成梯度)来更新权重W1。</p><p id="f24e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih jm"> Log() = &gt;简称</strong> <a class="ae jn" href="https://en.wikipedia.org/wiki/Logistic_function" rel="noopener ugc nofollow" target="_blank"> <strong class="ih jm">逻辑函数</strong> </a></p><p id="44ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上面的图像中有两个非常有趣的地方值得注意:第一，与常规的梯度更新相比，这个过程是多么的相似。第二，底部写的退货声明。别担心，我会解释的。</p><p id="656d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以下面是一些传统的方法来执行梯度更新。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jt"><img src="../Images/dcb39732424af7b29eadb68fe4e2e714.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8gK_x-TfHEexMRIqrd6etA.jpeg"/></div></div></figure><p id="276a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">获得成本函数相对于某一权重的导数有三个主要部分，首先使用链式法则，我们需要获得成本相对于某一层的输出的导数，某一层的输出相对于同一层的输入的导数，以及同一层的输入相对于权重的导数。现在将它与我们的前馈过程进行比较，它看起来有些什么非常相似！(这是显而易见的，但我仍然认为它有点酷)</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es kp"><img src="../Images/680f17696d9868499295a18d42943d91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ztxHf2WD8Pjd7AZCMaQJOA.jpeg"/></div></div></figure><p id="72ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在第二部分，返回语句，如果你读了iamtrask的原教程你会得到这部分。基本上，我们将每一层声明为一个类，我们希望使用下一层的输出来更新当前层的权重，因此我们返回一些数字。(这里不赘述数学，但是如果你看到L1WSG * W1_Transpose，这其实很有意义，非常类似于传统的渐变更新。)</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es ko"><img src="../Images/3e0a5b7451ab7648522c8e32b4031802.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*msVjd5s-eY5DD7AbQ2qyqg.png"/></div></div></figure><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jt"><img src="../Images/51e903ed5687bce063e063343f5d29f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bkuqtu0NFbxa6CBBpsYahA.jpeg"/></div></div></figure><p id="26dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们看看其余层的操作！</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jt"><img src="../Images/daf2f8808faa1a2586ecab78f57d01ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cnGOIFlSfWw6g4gVGqZSuA.jpeg"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx">Layer 2 operation</figcaption></figure><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jt"><img src="../Images/ed937429aad55b933f875d475e950a75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3eL5VXW7OZLVMUP7CSRIoA.jpeg"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx">Layer 3 Operation</figcaption></figure><p id="ecd8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">再次重申一下…</p><p id="ee19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih jm"> <em class="kf"> d1 - &gt;正向和合成更新<br/> d2 - &gt;更新合成权重<br/> e1 - &gt;正向和合成更新<br/> e2 - &gt;更新合成权重</em> </strong></p><p id="8e88" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是前馈操作和反向传播，然而这还不是全部！我们现在将观察解耦神经接口的真正力量，为什么它如此有用，以及这种力量来自哪里。</p></div><div class="ab cl kh ki gp kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="hb hc hd he hf"><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jt"><img src="../Images/45b1eb94aa04aee106950567bc2b9a67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*56xw9MRVho6EHhHiEq3iRw.jpeg"/></div></div></figure><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jt"><img src="../Images/1a16872c2708d3810ee34c4953b9085d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZmwrhJMBRVMxFqZVYqtJlQ.jpeg"/></div></div></figure><p id="15b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如你所注意到的，每一个操作都有一个数字和罗马数字符号与之相关，这是我们可以执行的操作顺序，这就是为什么合成渐变如此神奇的原因。在传统的神经网络中，每一层都是锁定的，不能异步训练。但是由于我们的网络是解耦的，我们可以以不同的顺序训练。我带你去看。</p><h2 id="93c1" class="ku kv hi bd kw kx ky kz la lb lc ld le iq lf lg lh iu li lj lk iy ll lm ln lo bi translated">红框排序:当我们<strong class="ak">不使用多线程</strong>时</h2><h2 id="ef3d" class="ku kv hi bd kw kx ky kz la lb lc ld le iq lf lg lh iu li lj lk iy ll lm ln lo bi translated"><strong class="ak">蓝盒子订购:当我们使用多线程时</strong></h2><h1 id="ab81" class="lp kv hi bd kw lq lr ls la lt lu lv le lw lx ly lh lz ma mb lk mc md me ln mf bi translated">红盒子订购案例:<strong class="ak">T5 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;6T7】</strong></h1><p id="77c2" class="pw-post-body-paragraph if ig hi ih b ii mh ik il im mi io ip iq mj is it iu mk iw ix iy ml ja jb jc hb bi translated">换句话说，在我们执行前馈操作之后，我们将把结果值传递给下一层，等待下一层返回一些值。使用下一层值，执行更新合成权重。例如，在计算完第1层之后，我们可以将计算的第1层直接给第2层，其中会给我们L2WSG * W2_Transpose。然后，我们可以毫不犹豫地对第1层执行更新合成权重！我们甚至没有等到最后一层(第三层)计算成本函数！这很好，但我们甚至可以做得更好！</p><h1 id="94e6" class="lp kv hi bd kw lq lr ls la lt lu lv le lw lx ly lh lz ma mb lk mc md me ln mf bi translated">蓝色盒子订购案例:I -&gt; II -&gt; III -&gt; IV -&gt; V</h1><p id="b068" class="pw-post-body-paragraph if ig hi ih b ii mh ik il im mi io ip iq mj is it iu mk iw ix iy ml ja jb jc hb bi translated">现在我们使用多线程，这意味着我们可以同时执行多个任务。让我们再看一个例子，在我们计算了第1层的值之后，我们将把该值提供给第2层，在第2层中，我们将再次得到L2WSG * W2_Transpose。在这里，我们可以生成一个新的进程来计算更新第1层的合成权重，同时将第2层的计算结果传递给第3层！这很性感。)</p></div><div class="ab cl kh ki gp kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="hb hc hd he hf"><h1 id="149e" class="lp kv hi bd kw lq mm ls la lt mn lv le lw mo ly lh lz mp mb lk mc mq me ln mf bi translated">交互式代码</h1><p id="c66c" class="pw-post-body-paragraph if ig hi ih b ii mh ik il im mi io ip iq mj is it iu mk iw ix iy ml ja jb jc hb bi translated">我修改了iamtrask的原始代码，以满足红框和蓝框的情况。我会马上给你代码，但是请务必阅读下一部分，红盒子案例与蓝盒子案例的结果。</p><p id="6000" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">来自我的GitHub的红盒子案例:<a class="ae jn" href="https://github.com/JaeDukSeo/Only_Numpy_Basic/blob/master/Decoupled_Neural_Network/1_Red_Box_Case.py" rel="noopener ugc nofollow" target="_blank">链接</a> <br/>来自Trinket.io的红盒子案例:<a class="ae jn" href="https://trinket.io/python3/4d02398c48" rel="noopener ugc nofollow" target="_blank">链接</a></p><p id="b395" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">来自我的Github的蓝盒子案例:<a class="ae jn" href="https://github.com/JaeDukSeo/Only_Numpy_Basic/blob/master/Decoupled_Neural_Network/2_Blue_Box_Case.py" rel="noopener ugc nofollow" target="_blank">链接</a> <br/>来自小饰品的蓝盒子案例. io: <a class="ae jn" href="https://trinket.io/python3/8ffd5cc56c" rel="noopener ugc nofollow" target="_blank">链接</a></p></div><div class="ab cl kh ki gp kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="hb hc hd he hf"><h2 id="8efa" class="ku kv hi bd kw kx ky kz la lb lc ld le iq lf lg lh iu li lj lk iy ll lm ln lo bi translated">红盒子vs蓝盒子</h2><p id="f22b" class="pw-post-body-paragraph if ig hi ih b ii mh ik il im mi io ip iq mj is it iu mk iw ix iy ml ja jb jc hb bi translated">理论上，产生一个子进程来训练网络听起来非常性感，但是我的实验产生了不同的结果。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es mr"><img src="../Images/cd3ed2f5048ce0d5d72a162f539d119c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jzb0nJd51sODjrKIe-Ut3g.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx">Red Box Training Time</figcaption></figure><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es ms"><img src="../Images/7d70388fc661b7a890440e82c34cc936.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-iQCdPl5gylNzncNTj1NLg.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx">Blue Box Training Time</figcaption></figure><p id="94bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如上所述，红盒子方法比蓝盒子方法要快很多(大约快98.27%)，这很令人失望，我真的不知道为什么会这样。不过好吧，我看完原文再回到这个问题！(如果你知道为什么会这样，请在下面的评论区留言！)</p></div><div class="ab cl kh ki gp kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="hb hc hd he hf"><h2 id="d385" class="ku kv hi bd kw kx ky kz la lb lc ld le iq lf lg lh iu li lj lk iy ll lm ln lo bi translated">最后的话</h2><p id="a43f" class="pw-post-body-paragraph if ig hi ih b ii mh ik il im mi io ip iq mj is it iu mk iw ix iy ml ja jb jc hb bi translated">再次感谢iamtrask，请访问他的博客<a class="ae jn" href="https://iamtrask.github.io" rel="noopener ugc nofollow" target="_blank">这里</a>并在twitter上关注他<a class="ae jn" href="https://twitter.com/iamtrask" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="ccc9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，在我的推特<a class="ae jn" href="https://twitter.com/JaeDukSeo" rel="noopener ugc nofollow" target="_blank">这里</a>关注我，并访问<a class="ae jn" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">我的网站</a>，或我的<a class="ae jn" href="https://www.youtube.com/c/JaeDukSeo" rel="noopener ugc nofollow" target="_blank"> Youtube频道</a>了解更多内容。如果你感兴趣，我还在简单的RNN <a class="ae jn" rel="noopener" href="/@SeoJaeDuk/only-numpy-vanilla-recurrent-neural-network-with-activation-deriving-back-propagation-through-time-4110964a9316">上做了反向传播。</a></p><p id="0fd7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是我在媒体上的第三篇文章，我对此很陌生，所以非常感谢建设性的批评，谢谢！(但是请注意，我从早上6点到11点写了这篇文章，因为我有严重的失眠问题，所以我的英语很差。)</p><p id="db39" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢阅读！</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es mt"><img src="../Images/731acf26f5d44fdc58d99a6388fe935d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6gfnVvkMRFtjVsWF7vkClA.png"/></div></div></figure><h2 id="fd41" class="ku kv hi bd kw kx ky kz la lb lc ld le iq lf lg lh iu li lj lk iy ll lm ln lo bi translated">这个故事发表在<a class="ae jn" href="https://medium.com/swlh" rel="noopener"> The Startup </a>上，这是Medium最大的企业家出版物，拥有292，582+人。</h2><h2 id="7945" class="ku kv hi bd kw kx ky kz la lb lc ld le iq lf lg lh iu li lj lk iy ll lm ln lo bi translated">在此订阅接收<a class="ae jn" href="http://growthsupply.com/the-startup-newsletter/" rel="noopener ugc nofollow" target="_blank">我们的头条新闻</a>。</h2><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es mt"><img src="../Images/731acf26f5d44fdc58d99a6388fe935d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6gfnVvkMRFtjVsWF7vkClA.png"/></div></div></figure></div></div>    
</body>
</html>
<html>
<head>
<title>At the Speed of Reinforcement Learning: an OpenAI Contest Story</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习的速度:一个开放人工智能竞赛的故事</h1>
<blockquote>原文：<a href="https://medium.com/swlh/at-the-speed-of-reinforcement-learning-an-openai-contest-story-6ed34fe7a3bb?source=collection_archive---------7-----------------------#2018-10-02">https://medium.com/swlh/at-the-speed-of-reinforcement-learning-an-openai-contest-story-6ed34fe7a3bb?source=collection_archive---------7-----------------------#2018-10-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/3a0a1aa1f6f10f578a964aed01ca079f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*AQTlKTmMa6T1Wto-AANMSg.gif"/></div></div></figure><figure class="ir is it iu fd ij er es paragraph-image"><div class="er es iq"><img src="../Images/62c16f6c0540649f8aaca095a27fb39d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*HtLaqnIUWz5cyMEOfSfuIQ.jpeg"/></div></figure><p id="25a5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">您好！我是谢尔盖·科列斯尼科夫，Dbrain的高级数据科学家，是</em><a class="ae ju" href="https://ml-mipt.github.io/index-spring-2018/" rel="noopener ugc nofollow" target="_blank"><em class="jt"/></a><em class="jt">和</em> <a class="ae ju" href="https://www.coursera.org/learn/practical-rl" rel="noopener ugc nofollow" target="_blank"> <em class="jt"> HSE </em> </a> <em class="jt">和</em> <a class="ae ju" href="https://github.com/Scitator/catalyst" rel="noopener ugc nofollow" target="_blank"> <em class="jt">开源</em> </a> <em class="jt">开发者的讲师。主要是对RL ( </em> <a class="ae ju" href="https://github.com/Scitator/Run-Skeleton-Run" rel="noopener ugc nofollow" target="_blank"> <em class="jt"> NIPS学习跑第三名</em> </a> <em class="jt">)，顺序决策和规划感兴趣。</em></p><p id="9cc2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Open AI今年春天举办了一场强化学习比赛——复古大赛。主要目标是提出一种元学习算法，可以将知识从“刺猬索尼克”的一组训练水平转移到OpenAI专门制定的一组以前看不到的测试水平。我们的团队在900多个团队中名列第四。强化学习不同于典型的机器学习，这次比赛从其他RL比赛中脱颖而出。你可以阅读下面的详细内容。</p><h1 id="c569" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">组</h1><p id="7ef8" class="pw-post-body-paragraph iv iw hi ix b iy kt ja jb jc ku je jf jg kv ji jj jk kw jm jn jo kx jq jr js hb bi translated">谢尔盖·科列斯尼科夫(<a class="ae ju" href="https://github.com/Scitator" rel="noopener ugc nofollow" target="_blank">剪辑师</a> ) <br/> RL爱好者。高级数据科学家@ <a class="ae ju" href="https://dbrain.io/" rel="noopener ugc nofollow" target="_blank"> Dbrain </a>。竞赛期间在<a class="ae ju" href="https://mipt.ru/english/" rel="noopener ugc nofollow" target="_blank"> MIPT </a>答辩过硕士论文。</p><p id="6b93" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">米哈伊尔·巴甫洛夫(<a class="ae ju" href="https://github.com/fgvbrt" rel="noopener ugc nofollow" target="_blank"> fgvbrt </a> ) <br/>高级研究工程师@ <a class="ae ju" href="https://reason8.ai/" rel="noopener ugc nofollow" target="_blank">理由8 </a>。定期RL竞赛/黑客马拉松参与者和奖金获得者。</p><p id="1d23" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">伊利亚·谢尔盖耶夫(<a class="ae ju" href="https://github.com/sergeevii123" rel="noopener ugc nofollow" target="_blank">谢尔盖耶维奇123 </a> ) <br/>数据科学家@<a class="ae ju" href="https://www.avito.ru/" rel="noopener ugc nofollow" target="_blank">avito . ru</a>——不同内部项目的计算机视觉。在<a class="ae ju" href="http://game.deephack.me" rel="noopener ugc nofollow" target="_blank"> Deephack开始RL。2015年游戏</a>。</p><p id="4a09" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Ivan Soro kin(<a class="ae ju" href="https://github.com/1ytic" rel="noopener ugc nofollow" target="_blank">1 lytic</a>)<br/>在<a class="ae ju" href="https://www.speechpro.ru/" rel="noopener ugc nofollow" target="_blank"> speechpro.ru </a>处理语音识别。</p><h1 id="f274" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated"><strong class="ak">强化学习和音波</strong></h1><figure class="ir is it iu fd ij er es paragraph-image"><div class="er es ky"><img src="../Images/829036092ad5efb1287621b6a5f83166.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KVF_o1FINHYsCQzC"/></div></figure><p id="82e6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">强化学习是一个理论和算法的集合体，它结合了机器学习和最优控制理论中的时间差分(TD)方法领域。在实践中，强化学习算法被用来识别难以数学定义的最优控制问题的解决方案。他们通过一个通过与环境互动从经验中学习的代理来实现这个目标。环境为代理人提供基于其行为的奖励；代理人的行为越好，得到的回报就越高。因此，好的控制者是通过让代理人学会通过执行最佳行动来最大化所收到的回报而获得的。</p><figure class="ir is it iu fd ij er es paragraph-image"><div class="er es kz"><img src="../Images/96a97a4e1059710d940087cc3d65f766.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/0*p_2DC6_jLP5U9pKK"/></div></figure><p id="71ad" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在声音比赛中，RGB图像充当环境，作为一个可用的动作，代理必须选择虚拟控制器上的按钮。类似于原来的游戏，奖励增加了戒指收集和水平通过速度。基本上，我们有了以我们的代理人为主角的原始索尼克游戏。</p><h1 id="18e3" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated"><strong class="ak">基线</strong></h1><p id="1ec3" class="pw-post-body-paragraph iv iw hi ix b iy kt ja jb jc ku je jf jg kv ji jj jk kw jm jn jo kx jq jr js hb bi translated">作为基线，我们为<a class="ae ju" href="https://arxiv.org/abs/1710.02298" rel="noopener ugc nofollow" target="_blank">彩虹</a> (DQN方法)和<a class="ae ju" href="https://arxiv.org/abs/1707.06347" rel="noopener ugc nofollow" target="_blank"> PPO </a>(政策梯度方法)代理提供了完整的指南，指导他们在一个可能的声波水平上进行培训，并最终提交代理。Rainbow版本基于一个anyrl项目，而PPO使用了众所周知的OpenAI基线。公布的基线不同于<a class="ae ju" href="https://arxiv.org/abs/1804.03720" rel="noopener ugc nofollow" target="_blank">文章</a>中描述的基线——它们更简单，学习加速的次数也更少。</p><figure class="ir is it iu fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es la"><img src="../Images/d45233a0ff046841a666facc79205eb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WCWhTgR1dF21G1Ln"/></div></div></figure><p id="ebb3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix lb">结果评估功能</strong></p><p id="bd6c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">算法在与它们在典型的RL竞赛中学习到的环境相同的环境中被测试。这有利于擅长记忆和具有许多超参数的算法。在这场比赛中，代理人在新索尼克的水平上进行测试，这些水平是由开放AI团队为比赛设计的。此外，代理可以在测试时获得奖励，这使得微调成为可能。然而，要记住测试的时间限制:我们有24小时和最多100万次滴答。</p><p id="bf4d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这次比赛中，团队以docker图像和组装的API提供结果。这种策略解决方案检索更公平，因为资源和时间受到docker映像的限制。我真的很欣赏这种方法，因为它让那些缺乏“DGX和AWS家庭集群”的研究人员处于与最后一批9000多名模特爱好者相同的条件下。</p><p id="14e2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">顺便说一下，在<a class="ae ju" href="https://dbrain.io/" rel="noopener ugc nofollow" target="_blank"> Dbrain </a>使用了相同的竞争策略。我们试图实现的主要目标是有洞察力的模型开发。从我们的竞赛参与者那里获得的解决方案保存在带有集成API的docker image中。多亏了它，我们可以得到预测和求解过程。真的希望以后能看到更多这样的比赛。</p><figure class="ir is it iu fd ij"><div class="bz dy l di"><div class="lc ld l"/></div></figure><h1 id="753d" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated"><strong class="ak">方法和结果</strong></h1><p id="081e" class="pw-post-body-paragraph iv iw hi ix b iy kt ja jb jc ku je jf jg kv ji jj jk kw jm jn jo kx jq jr js hb bi translated">在快速回顾了建议的基线后，我们选择了OpenAI的PPO方法，作为我们未来解决方案的一种更正式和有趣的方式。通过OpenAI <a class="ae ju" href="https://arxiv.org/abs/1804.03720" rel="noopener ugc nofollow" target="_blank">技术报告</a>PPO代理更好地处理了任务。所以，我们的主要特点:</p><h2 id="0031" class="le jw hi bd jx lf lg lh kb li lj lk kf jg ll lm kj jk ln lo kn jo lp lq kr lr bi translated">1.PPO联合培训</h2><figure class="ir is it iu fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ls"><img src="../Images/70a87d1452dad4959b902c81a74225c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sTIWVzkNVZaeTfLo"/></div></div></figure><p id="74bb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们得到的基线只能学习27个音速等级中的一个。但是我们修改了学习过程，将它平行应用于所有27个级别。因此该智能体具有更高的泛化能力和更好的声音世界定向能力。</p><h2 id="c8f8" class="le jw hi bd jx lf lg lh kb li lj lk kf jg ll lm kj jk ln lo kn jo lp lq kr lr bi translated">2.测试过程中的微调</h2><p id="04bf" class="pw-post-body-paragraph iv iw hi ix b iy kt ja jb jc ku je jf jg kv ji jj jk kw jm jn jo kx jq jr js hb bi translated">我们必须根据竞赛的主旨找到最具概括性的方法。这需要代理人微调其测试水平的政策，通常这就是我们所做的——在每一集/游戏结束时，代理人估计奖励并进一步改进其政策以最大化奖励预期。</p><h2 id="962b" class="le jw hi bd jx lf lg lh kb li lj lk kf jg ll lm kj jk ln lo kn jo lp lq kr lr bi translated">3.探险奖金</h2><p id="b509" class="pw-post-body-paragraph iv iw hi ix b iy kt ja jb jc ku je jf jg kv ji jj jk kw jm jn jo kx jq jr js hb bi translated">现在让我们深入了解等级奖励条件。代理获得了x坐标进度的奖励，因此在需要前进和后退时可能会被卡住。因此，我们设立了一个名为<a class="ae ju" href="https://arxiv.org/abs/1606.01868" rel="noopener ugc nofollow" target="_blank">基于计数的探索</a>的额外奖励，给出了代理可能进入的新状态。</p><figure class="ir is it iu fd ij er es paragraph-image"><div class="er es lt"><img src="../Images/a747482567160c0ee704d5ab61dd9667.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/0*3HqKhaGwfIc5GEfT"/></div></figure><p id="63a5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">探索奖励以两种形式实现:基于像素相似性的图像，以及基于特定位置的状态频率的x坐标。这两者都是与代理人访问的条件的单一性相反地形成的。</p><h2 id="7360" class="le jw hi bd jx lf lg lh kb li lj lk kf jg ll lm kj jk ln lo kn jo lp lq kr lr bi translated">4.最佳初始策略搜索</h2><p id="195f" class="pw-post-body-paragraph iv iw hi ix b iy kt ja jb jc ku je jf jg kv ji jj jk kw jm jn jo kx jq jr js hb bi translated">这一改进大大有助于结果。这个想法非常简单:我们用不同的超参数训练了几个策略。在测试的时候，每个策略都在前几集进行了测试。然后我们选择最好的一个做进一步的微调。</p><h1 id="057b" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">失误</h1><p id="206f" class="pw-post-body-paragraph iv iw hi ix b iy kt ja jb jc ku je jf jg kv ji jj jk kw jm jn jo kx jq jr js hb bi translated">什么没玩好:</p><ol class=""><li id="e284" class="lu lv hi ix b iy iz jc jd jg lw jk lx jo ly js lz ma mb mc bi translated">NN架构变化:<a class="ae ju" href="https://arxiv.org/abs/1706.02515" rel="noopener ugc nofollow" target="_blank"> SELU激活</a>，自我关注，<a class="ae ju" href="https://arxiv.org/abs/1709.01507" rel="noopener ugc nofollow" target="_blank"> SE屏蔽</a></li><li id="0ee7" class="lu lv hi ix b iy md jc me jg mf jk mg jo mh js lz ma mb mc bi translated"><a class="ae ju" href="https://arxiv.org/abs/1712.06567" rel="noopener ugc nofollow" target="_blank">神经进化</a></li><li id="6e2a" class="lu lv hi ix b iy md jc me jg mf jk mg jo mh js lz ma mb mc bi translated">个人声波的水平创造-我们已经准备了整个管道，但没有足够的时间来实现它</li><li id="fb8d" class="lu lv hi ix b iy md jc me jg mf jk mg jo mh js lz ma mb mc bi translated">像<a class="ae ju" href="https://arxiv.org/abs/1703.03400" rel="noopener ugc nofollow" target="_blank"> MAML </a>和<a class="ae ju" href="https://arxiv.org/abs/1803.02999" rel="noopener ugc nofollow" target="_blank">爬虫</a>这样的元学习算法</li><li id="7025" class="lu lv hi ix b iy md jc me jg mf jk mg jo mh js lz ma mb mc bi translated">基于重要抽样的模型集成</li></ol><figure class="ir is it iu fd ij"><div class="bz dy l di"><div class="mi ld l"/></div></figure><h1 id="6a2d" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated"><strong class="ak">结果</strong></h1><p id="13e1" class="pw-post-body-paragraph iv iw hi ix b iy kt ja jb jc ku je jf jg kv ji jj jk kw jm jn jo kx jq jr js hb bi translated">OpenAI在比赛结束三周后公布了<a class="ae ju" href="https://blog.openai.com/first-retro-contest-retrospective/" rel="noopener ugc nofollow" target="_blank">结果</a>。我们的团队在11个附加级别中获得了第4名，在公开测试中从第8名跃升，并超过了调整后的OpenAI基线。</p><figure class="ir is it iu fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/28159fa9e7f5420cc89dfbc932f7d46b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/0*elf0rhaTF7n7JCsv"/></div></figure><p id="269b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">来自三大解决方案的超酷功能:</p><ol class=""><li id="5502" class="lu lv hi ix b iy iz jc jd jg lw jk lx jo ly js lz ma mb mc bi translated">更常见的按钮组合增加了操作空间</li><li id="dff1" class="lu lv hi ix b iy md jc me jg mf jk mg jo mh js lz ma mb mc bi translated">基于屏幕感知哈希的探索奖励</li><li id="881e" class="lu lv hi ix b iy md jc me jg mf jk mg jo mh js lz ma mb mc bi translated">来自游戏男孩高级和主系统声波游戏的更多训练水平</li></ol><p id="b03d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">你可以得到更详细的描述<a class="ae ju" href="https://blog.openai.com/first-retro-contest-retrospective/" rel="noopener ugc nofollow" target="_blank">这里</a>里面有排名前三的源代码。根据竞赛的最佳实践，全部代码可在<a class="ae ju" href="https://github.com/fgvbrt/retro_contest" rel="noopener ugc nofollow" target="_blank"> GitHub上获得。</a></p><p id="a233" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">顺便说一下，我很感激OpenAI也鼓励了<a class="ae ju" rel="noopener" href="/@olegmrk/openai-retro-contest-report-b870bfd014e0">最佳报道</a>音轨。</p><p id="449f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">欢迎随时评论提问！</p><p id="e211" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae ju" href="https://dbrain.io" rel="noopener ugc nofollow" target="_blank"> dbrain.io </a></p><p id="9bc2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix lb">加入</strong> <a class="ae ju" href="https://t.me/Dbrainchat" rel="noopener ugc nofollow" target="_blank"> <strong class="ix lb">电报</strong> </a> <strong class="ix lb">聊天，关注我们上</strong> <a class="ae ju" href="https://twitter.com/dbrainio" rel="noopener ugc nofollow" target="_blank"> <strong class="ix lb">推特</strong> </a> <strong class="ix lb">，喜欢我们上</strong> <a class="ae ju" href="https://www.facebook.com/dbrainio/" rel="noopener ugc nofollow" target="_blank"> <strong class="ix lb">脸书</strong> </a></p><figure class="ir is it iu fd ij er es paragraph-image"><a href="https://medium.com/swlh"><div class="er es mk"><img src="../Images/308a8d84fb9b2fab43d66c117fcc4bb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YqDjlKFwScoQYQ62DWEdig.png"/></div></a></figure><h2 id="c148" class="le jw hi bd jx lf lg lh kb li lj lk kf jg ll lm kj jk ln lo kn jo lp lq kr lr bi translated">这篇文章发表在<a class="ae ju" href="https://medium.com/swlh" rel="noopener"> The Startup </a>上，这是Medium最大的创业刊物，拥有+ 374，357名读者。</h2><h2 id="9478" class="le jw hi bd jx lf lg lh kb li lj lk kf jg ll lm kj jk ln lo kn jo lp lq kr lr bi translated">在这里订阅接收<a class="ae ju" href="http://growthsupply.com/the-startup-newsletter/" rel="noopener ugc nofollow" target="_blank">我们的头条新闻</a>。</h2><figure class="ir is it iu fd ij er es paragraph-image"><a href="https://medium.com/swlh"><div class="er es mk"><img src="../Images/b0164736ea17a63403e660de5dedf91a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ouK9XR4xuNWtCes-TIUNAw.png"/></div></a></figure></div></div>    
</body>
</html>